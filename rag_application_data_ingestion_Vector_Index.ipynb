{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhardwaj-Saurabh/personal_knowledge_assistant/blob/master/rag_application_data_ingestion_Vector_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "xuRbkfk_qGX5"
      },
      "outputs": [],
      "source": [
        "# !pip install -r /content/drive/MyDrive/FInananceRagAgent/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !playwright install"
      ],
      "metadata": {
        "id": "jruPoCnQxTtQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BjvGVHjqSa1"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Annotated\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "sbRW5kuO1sR6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zTdJQoNkrPPJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "HUGGINGFACE_ACCESS_TOKEN = userdata.get('HF_TOKEN')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "boEajKb-3Win"
      },
      "outputs": [],
      "source": [
        "class DocumentMetadata(BaseModel):\n",
        "    id: str\n",
        "    url: str\n",
        "    title: str\n",
        "    properties: dict\n",
        "\n",
        "    def obfuscate(self) -> \"DocumentMetadata\":\n",
        "        original_id = self.id.replace(\"-\", \"\")\n",
        "        fake_id = generate_random_hex(len(original_id))\n",
        "\n",
        "        self.id = fake_id\n",
        "        self.url = self.url.replace(original_id, fake_id)\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v3b__XGS3Wio"
      },
      "outputs": [],
      "source": [
        "def generate_random_hex(length: int) -> str:\n",
        "    hex_chars = string.hexdigits.lower()\n",
        "    return \"\".join(random.choice(hex_chars) for _ in range(length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yoqH-sbu3Wip"
      },
      "outputs": [],
      "source": [
        "class Document(BaseModel):\n",
        "    id: str = Field(default_factory=lambda: generate_random_hex(length=32))\n",
        "    metadata: DocumentMetadata\n",
        "    parent_metadata: DocumentMetadata | None = None\n",
        "    content: str\n",
        "    content_quality_score: float | None = None\n",
        "    summary: str | None = None\n",
        "    child_urls: list[str] = Field(default_factory=list)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file_path: Path) -> \"Document\":\n",
        "        json_data = file_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "        return cls.model_validate_json(json_data)\n",
        "\n",
        "    def add_summary(self, summary: str) -> \"Document\":\n",
        "        self.summary = summary\n",
        "\n",
        "        return self\n",
        "\n",
        "    def add_quality_score(self, score: float) -> \"Document\":\n",
        "        self.content_quality_score = score\n",
        "\n",
        "        return self\n",
        "\n",
        "    def write(\n",
        "        self, output_dir: Path, obfuscate: bool = False, also_save_as_txt: bool = False\n",
        "    ) -> None:\n",
        "\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if obfuscate:\n",
        "            self.obfuscate()\n",
        "\n",
        "        json_page = self.model_dump()\n",
        "\n",
        "        output_file = output_dir / f\"{self.id}.json\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(\n",
        "                json_page,\n",
        "                f,\n",
        "                indent=4,\n",
        "                ensure_ascii=False,\n",
        "            )\n",
        "\n",
        "        if also_save_as_txt:\n",
        "            txt_path = output_file.with_suffix(\".txt\")\n",
        "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(self.content)\n",
        "\n",
        "    def obfuscate(self) -> \"Document\":\n",
        "        self.metadata = self.metadata.obfuscate()\n",
        "        self.parent_metadata = (\n",
        "            self.parent_metadata.obfuscate() if self.parent_metadata else None\n",
        "        )\n",
        "        self.id = self.metadata.id\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __eq__(self, other: object) -> bool:\n",
        "        if not isinstance(other, Document):\n",
        "            return False\n",
        "        return self.id == other.id\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(self.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R6h4Ly7I3Wip"
      },
      "outputs": [],
      "source": [
        "def __get_json_files(data_directory: Path, nesting_level: int = 0) -> list[Path]:\n",
        "    if nesting_level == 0:\n",
        "        return list(data_directory.glob(\"*.json\"))\n",
        "    else:\n",
        "        json_files = []\n",
        "        for database_dir in data_directory.iterdir():\n",
        "            if database_dir.is_dir():\n",
        "                nested_json_files = __get_json_files(\n",
        "                    data_directory=database_dir, nesting_level=nesting_level - 1\n",
        "                )\n",
        "                json_files.extend(nested_json_files)\n",
        "\n",
        "        return json_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pFKdqt66qrom"
      },
      "outputs": [],
      "source": [
        "COLLECTION_NAME = 'knowledge_assistant'\n",
        "MONGODB_DATABASE_NAME = 'knowledgeassistantdatabase'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c6O1ln-jq_k0"
      },
      "outputs": [],
      "source": [
        "from typing import Generic, Type, TypeVar\n",
        "\n",
        "from bson import ObjectId\n",
        "from loguru import logger\n",
        "from pydantic import BaseModel\n",
        "from pymongo import MongoClient, errors\n",
        "\n",
        "T = TypeVar(\"T\", bound=BaseModel)\n",
        "\n",
        "\n",
        "class MongoDBService(Generic[T]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Type[T],\n",
        "        collection_name: str = COLLECTION_NAME,\n",
        "        database_name: str = MONGODB_DATABASE_NAME,\n",
        "        mongodb_uri: str = MONGODB_URI,\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.collection_name = collection_name\n",
        "        self.database_name = database_name\n",
        "        self.mongodb_uri = mongodb_uri\n",
        "\n",
        "        try:\n",
        "            self.client = MongoClient(mongodb_uri)\n",
        "            self.client.admin.command(\"ping\")\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "        self.database = self.client[database_name]\n",
        "        self.collection = self.database[collection_name]\n",
        "\n",
        "    def __enter__(self) -> \"MongoDBService\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n",
        "        self.close()\n",
        "\n",
        "    def clear_collection(self) -> None:\n",
        "        try:\n",
        "            result = self.collection.delete_many({})\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def ingest_documents(self, documents: list[T]) -> None:\n",
        "        try:\n",
        "            if not documents or not all(\n",
        "                isinstance(doc, BaseModel) for doc in documents\n",
        "            ):\n",
        "                raise ValueError(\"Documents must be a list of Pycantic models.\")\n",
        "\n",
        "            dict_documents = [doc.model_dump() for doc in documents]\n",
        "\n",
        "            # Remove '_id' fields to avoid duplicate key errors\n",
        "            for doc in dict_documents:\n",
        "                doc.pop(\"_id\", None)\n",
        "\n",
        "            self.collection.insert_many(dict_documents)\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def fetch_documents(self, limit: int, query: dict) -> list[T]:\n",
        "        try:\n",
        "            documents = list(self.collection.find(query).limit(limit))\n",
        "            return self.__parse_documents(documents)\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "    def __parse_documents(self, documents: list[dict]) -> list[T]:\n",
        "        parsed_documents = []\n",
        "        for doc in documents:\n",
        "            for key, value in doc.items():\n",
        "                if isinstance(value, ObjectId):\n",
        "                    doc[key] = str(value)\n",
        "\n",
        "            _id = doc.pop(\"_id\", None)\n",
        "            doc[\"id\"] = _id\n",
        "\n",
        "            parsed_doc = self.model.model_validate(doc)\n",
        "            parsed_documents.append(parsed_doc)\n",
        "\n",
        "        return parsed_documents\n",
        "\n",
        "    def get_collection_count(self) -> int:\n",
        "        try:\n",
        "            return self.collection.count_documents({})\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.client.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fatch Data from MongoDB"
      ],
      "metadata": {
        "id": "DB49jhmwpG0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "CEYT0dfrqSYQ"
      },
      "outputs": [],
      "source": [
        "def fetch_from_mongodb(\n",
        "    collection_name: str,\n",
        "    limit: int,\n",
        ") -> Annotated[list[dict], \"documents\"]:\n",
        "    with MongoDBService(model=Document, collection_name=collection_name) as service:\n",
        "        documents = service.fetch_documents(limit, query={})\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "-JPFqsePqSVb"
      },
      "outputs": [],
      "source": [
        "documents = fetch_from_mongodb(\n",
        "    collection_name = COLLECTION_NAME,\n",
        "    limit = 1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCiAXvYNhFK_",
        "outputId": "91a8ca5a-7583-4c83-c1db-4a6d91f985cb"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "474"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6wqm7yGqSRR",
        "outputId": "69c735be-b927-43e1-ac74-90b04755b372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[Lightning AI Studios: Never set up a local environment again →](https://lightning.ai)\n**\nLog in or create a free Lightning.ai account to track your progress and access additional course materials[Get Started →](https://lightning.ai/sign-in?redirectTo=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F)\n**\nDeep Learning Fundamentals \n* Pages\n  * [Deep Learning Fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/)\n    * [Unit 1Intro to ML and DL](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/)\n      * [Unit 1.1What Is ML?](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n      * [Unit 1.2How Can We Use ML?](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-2/)\n      * [Unit 1.3A Typical ML Workflow](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-3/)\n      * [Unit 1.4The First ML Classifier](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/ml-classifier/)\n      * [Unit 1.5Setting Up Our Computing Environment](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-5-computing-environment/)\n      * [Unit 1.6Implementing a Perceptron in Python](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-6-implementing-a-perceptron-in-python-parts-1-3/)\n      * [Unit 1.7 Evaluating Machine Learning Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-7-evaluating-machine-learning-models-parts-1-and-2/)\n      * [Unit 1 ExercisesUnit 1 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/exercises/)\n    * [Unit 2Using Tensors w/ PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/)\n      * [Unit 2.1Pytorch Intro](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-1-introducing-pytorch/)\n      * [Unit 2.2Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-2-what-are-tensors-part-1-and-part-2/)\n      * [Unit 2.3Using Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-3-how-do-we-use-tensors-in-pytorch/)\n      * [Unit 2.4Linear Algebra](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-4-improving-code-efficiency-with-linear-algebra-parts-1-4/)\n      * [Unit 2.5Debugging Code](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-5-debugging-code/)\n      * [Unit 2.6Revisiting Perceptron w/ Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-6-revisiting-the-perceptron-algorithm/)\n      * [Unit 2.7Computation Graphs](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-7-seeing-predictive-models-as-computation-graphs/)\n      * [Unit 2 ExercisesExercises](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/unit-2-exercises/)\n    * [Unit 3Model Training in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/)\n      * [Unit 3.1Using Logistic Regression for Classification](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-1-using-logistic-regression-for-classification-parts-1-3/)\n      * [Unit 3.2The Logistic Regression Computation Graph](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-2-the-logistic-regression-computation-graph/)\n      * [Unit 3.3Model Training with Stochastic Gradient Descent ](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/)\n      * [Unit 3.4Automatic Differentiation in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/)\n      * [Unit 3.5The PyTorch API](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/)\n      * [Unit 3.6Training a Logistic Regression Model in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-6-training-a-logistic-regression-model-in-pytorch-parts-1-3/)\n      * [Unit 3.7 Feature Normalization](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-7-feature-normalization-parts-1-2/)\n      * [Unit 3 ExercisesUnit 3 Exercies](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/unit-3-exercises/)\n    * [Unit 4Training Multilayer Neural Networks Overview](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/)\n      * [Unit 4.1Logistic Regression for Multiple Classes](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/)\n      * [Unit 4.2Multilayer Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-2-multilayer-neural-networks-part-1-3/)\n      * [Unit 4.3Training a Multilayer Neural Network in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-3-training-a-multilayer-neural-network-in-pytorch-part-1-5/)\n      * [Unit 4.4Defining Efficient Data Loaders](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-4-defining-efficient-data-loaders-part-1-4/)\n      * [Unit 4.5Multilayer Neural Networks for Regression](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-5-multilayer-neural-networks-for-regression-parts-1-2/)\n      * [Unit 4.6Speeding Up Model Training Using GPUs](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-6-speeding-up-model-training-using-gpus/)\n      * [Unit 4 ExercisesUnit 4 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/unit-4-exercises/)\n    * [Unit 5Organizing Your Code with Lightning](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/)\n      * [Unit 5.1 Organizing Your Code with Lightning](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/organizing-your-code-with-lightning/)\n      * [Unit 5.2Training a Multilayer Perceptron using the Lightning Trainer](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-2-training-a-multilayer-perceptron-using-the-lightning-trainer/)\n      * [Unit 5.3Computing Metrics Efficiently with TorchMetrics](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-3-computing-metrics-efficiently-with-torchmetrics/)\n      * [Unit 5.4Making Code Reproducible](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-4-making-code-reproducible/)\n      * [Unit 5.5Organizing Your Data Loaders with Data Modules](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-5-organizing-your-data-loaders-with-data-modules/)\n      * [Unit 5.6The Benefits of Logging Your Model Training](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-6-the-benefits-of-logging-your-model-training/)\n      * [Unit 5.7Evaluating and Using Models on New Data](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-7-evaluating-and-using-models-on-new-data/)\n      * [Unit 5.8Add Functionality with Callbacks](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-8-add-functionality-with-callbacks/)\n      * [Unit 5 ExercisesUnit 5 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/unit-5-exercises/)\n    * [Unit 6Essential Deep Learning Tips & Tricks](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/)\n      * [Unit 6.1 Model Checkpointing and Early Stopping](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.1-model-checkpointing-and-early-stopping/)\n      * [Unit 6.2Learning Rates and Learning Rate Schedulers](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/)\n      * [Unit 6.3Using More Advanced Optimization Algorithms](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.3-using-more-advanced-optimization-algorithms/)\n      * [Unit 6.4Choosing Activation Functions](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.4-choosing-activation-functions/)\n      * [Unit 6.5Automating The Hyperparameter Tuning Process](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.5-automating-the-hyperparameter-tuning-process/)\n      * [Unit 6.6Improving Convergence with Batch Normalization](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.6-improving-convergence-with-batch-normalization/)\n      * [Unit 6.7Reducing Overfitting With Dropout](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/6.7-reducing-overfitting-with-dropout/)\n      * [Unit 6.8Debugging Deep Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/6.8-debugging-deep-neural-networks/)\n      * [Unit 6 ExercisesUnit 6 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6-exercises/)\n    * [Unit 7Getting Started with Computer Vision](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/)\n      * [Unit 7.1Working With Images](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.1-working-with-images/)\n      * [Unit 7.2How Convolutional Neural Networks Work](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.2-how-convolutional-neural-networks-work/)\n      * [Unit 7.3Convolutional Neural Network Architectures](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.3-convolutional-neural-network-architectures/)\n      * [Unit 7.4Training Convolutional Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.4-training-convolutional-neural-networks/)\n      * [Unit 7.5Improving Predictions with Data Augmentation](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.5-improving-predictions-with-data-augmentation/)\n      * [Unit 7.6Leveraging Pretrained Models with Transfer Learning](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.6-leveraging-pretrained-models-with-transfer-learning/)\n      * [Unit 7.7Using Unlabeled Data with Self-Supervised](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.7-using-unlabeled-data-with-self-supervised/)\n      * [Unit 7 ExercisesUnit 7 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7-exercises/)\n    * [Unit 8Natural Language Processing and Large Language Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/)\n      * [Unit 8.1Working with Text Data](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.1-working-with-text-data/)\n      * [Unit 8.2Training A Text Classifier Baseline](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.2-training-a-text-classifier-baseline/)\n      * [Unit 8.3Introduction to Recurrent Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.3-introduction-to-recurrent-neural-networks/)\n      * [Unit 8.4From RNNs to the Transformer Architecture](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/)\n      * [Unit 8.5Understanding Self-Attention](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.5-understanding-self-attention/)\n      * [Unit 8.6Large Language Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.6-large-language-models/)\n      * [Unit 8.7A Large Language Model for Classification](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.7-a-large-language-model-for-classification/)\n      * [Unit 8 ExercisesUnit 8 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/unit-8-exercises/)\n    * [Unit 9Techniques for Speeding Up Model Training](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/)\n      * [Unit 9.1Accelerated Model Training via Mixed-Precision Training](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.1-accelerated-model-training-via-mixed-precision-training/)\n      * [Unit 9.2Multi-GPU Training Strategies](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.2-multi-gpu-training-strategies/)\n      * [Unit 9.3Deep Dive Into Data Parallelism](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.3-deep-dive-into-data-parallelism/)\n      * [Unit 9.4Compiling PyTorch Models](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.4-compiling-pytorch-models/)\n      * [Unit 9.5Increasing Batch Sizes to Increase Throughput ](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.5-increasing-batch-sizes-to-increase-throughput/)\n      * [Unit 9 ExercisesUnit 9 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9-exercises/)\n    * [Unit 10 The Finale: Our Next Steps After AI Model Training](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/)\n      * [Unit 10.1Trustworthy and Reliable Machine Learning](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.1-trustworthy-and-reliable-machine-learning/)\n      * [Unit 10.2Scaling PyTorch Models without Boilerplate Code](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.2-fabric-scaling-pytorch-models-without-boilerplate-code/)\n      * [Unit 10.3Designing Machine Learning Systems](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.3-designing-machine-learning-systems/)\n      * [Unit 10.4Conclusion](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.4-conclusion/)\n      * [Unit 10 ExercisesUnit 10 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/unit-10-exercises/)\n\n\n[Final certification exam](https://lightning.ai/ai-education/deep-learning-fundamentals/certification/)\n[Deep Learning Fundamentals](https://lightning.ai/pages/courses/deep-learning-fundamentals/) > Deep Learning Fundamentals\n  * Share:\n  * [![Tweet](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/twitter.svg)](https://twitter.com/intent/tweet?source=http%3A%2F%2Flightning.ai&text=:%20http%3A%2F%2Flightning.ai&via=LightningAI \"Tweet\")\n  * [![Submit to Reddit](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/reddit.svg)](http://www.reddit.com/submit?url=http%3A%2F%2Flightning.ai&title= \"Submit to Reddit\")\n  * [![Share on LinkedIn](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/linkedin.svg)](http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flightning.ai&title=&summary=&source=http%3A%2F%2Flightning.ai \"Share on LinkedIn\")\n\n\nCourse Progress:\n#  Deep Learning Fundamentals\n[Start Course](https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n## Welcome to Deep Learning Fundamentals\nDeep Learning Fundamentals is a free course on learning deep learning using a modern open-source stack.\nIf you found this page, you probably heard that artificial intelligence and deep learning are taking the world by storm. This is correct. In this course, [Sebastian Raschka](https://sebastianraschka.com/), a best-selling author and professor, will teach you deep learning (machine learning with deep learning) from the ground up via a course of 10 units with bite-sized videos, quizzes, and exercises. The entire course is free and uses the most popular open-source tools for deep learning.\n**What will you learn in this course?**\n  * What machine learning is and when to use it\n  * The main concepts of deep learning\n  * How to design deep learning experiments with PyTorch\n  * How to write efficient deep learning code with PyTorch Lightning\n\n\n**What will you be able to do after this course?**\n  * Build classifiers for various kinds of data like tables, images, and text\n  * Tune models effectively to optimize predictive and computational performance\n\n\n**How is this course structured?**\n  * The course consists of 10 units, each containing several subsections\n  * It is centered around informative, succinct videos that are respectful of your time\n  * In each unit, you will find optional exercises to practice your knowledge\n  * We also provide additional resources for those who want a deep dive on specific topics\n\n\n**What are the prerequisites?**\n  * Ideally, you should already be familiar with programming in Python\n  * (Some lectures will involve a tiny bit of math, but a strong math background is not required!)\n\n\n**Are there interactive quizzes or exercises?**\n  * Each section is accompanied by optional multiple-choice quizzes to test your understanding of the material\n  * Optionally, each unit also features one or more code exercises to practice implementing concepts covered in this class\n\n\n**Is there a course completion badge or certificate?**\n  * At the end of this course, you can take an optional exam featuring 25 multiple-choice questions\n  * Upon answering 80% of the questions in the exam correctly (there are 5 attempts), you obtain an optional course completion badge that can be shared on LinkedIn\n\n[Start Course](https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n**Log in or create a free Lightning.ai account to access:**\n  * Quizzes\n  * Completion badges\n  * Progress tracking\n  * Additional downloadable content\n  * Additional AI education resources\n  * Notifications when new units are released\n  * Free cloud computing credits\n\n[Sign Up or Log In](https://lightning.ai/sign-in?redirectTo=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F)\n#####  Watch Video 1\n##### Videos\n![](https://t.co/1/i/adsct?=&bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%261080%26600%262%2624%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=ced468f8-e912-4e7d-88f9-ca7345132140&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=8c30a3f6-c71f-4138-93b4-02957c8b251a&tw_document_href=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.33)![](https://analytics.twitter.com/1/i/adsct?=&bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%261080%26600%262%2624%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=ced468f8-e912-4e7d-88f9-ca7345132140&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=8c30a3f6-c71f-4138-93b4-02957c8b251a&tw_document_href=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.33)\n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(documents[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter the documents by a predefined Quality Score"
      ],
      "metadata": {
        "id": "vHOv_kDupR7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "valid_docs = [doc for doc in documents\n",
        "        if not doc.content_quality_score\n",
        "        or doc.content_quality_score > 0.4\n",
        "    ]"
      ],
      "metadata": {
        "id": "mTYtHSOzhT3u"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFAMzgCMhj9T",
        "outputId": "48e7fa47-d63b-48a6-8f8d-4b61de601897"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "436"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_docs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFhpEkSOhbz5",
        "outputId": "1076a4b7-b9ac-4e0b-d131-7f45bd8c777f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(id='683d9733440aa39ed259cb2e', metadata=DocumentMetadata(id='d49bb7ceb2fb9cdea9d9fe80eebfad6b', url='https://www.langflow.org/', title='', properties={'description': None, 'keywords': None, 'author': None}), parent_metadata=DocumentMetadata(id='75e8df3f4fbc9df8a04a8f9a3fc903af', url='https://www.notion.so/Frameworks-75e8df3f4fbc9df8a04a8f9a3fc903af', title='Frameworks', properties={'Type': 'Leaf'}), content=\"###### Langflow Desktop for macOS is here!\\n[Try the alpha](https://www.langflow.org/desktop)\\n[![Langflow Logo](https://www.langflow.org/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.f6f14a6e.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)](https://www.langflow.org/)\\n###### [Docs](http://docs.langflow.org/)\\n###### Resources\\n###### Community\\n###### [62k](https://github.com/langflow-ai/langflow)\\n###### [12k](https://discord.com/invite/EqksyE2EX9)\\n###### [8k](https://x.com/langflow_ai)\\n###### [6k](https://www.youtube.com/@Langflow)\\n# Stop fighting your tools\\nLangflow is a low-code builder that makes it easier to build powerful AIs that can use any API, model, or database.\\nGet Started for FreeStar on GitHub\\nInput\\nModel\\nl3\\nllama-3.2\\nAPI Key\\nTemperature\\n0.5\\nPrecise\\nCreative\\n##### Used by leading AI development teams\\n![gradient](https://www.langflow.org/images/Gradient.png?w=16&q=75)\\n“Langflow lets us take complex product ideas and quickly bring them to life through visual flows that anyone can understand.”\\n![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_2.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\nJonathan Blomgren\\nCOO, BetterUp\\nDitch the Black Boxes\\nModel\\nl3\\nllama-3.2\\nTemperature\\n0.5\\nPrecise\\nCreative\\nResponse Length\\nShort\\nMedium\\nLong\\nControl the complexity\\n![Chat](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-chat.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n![Model](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-model.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n![Agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-agent.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\nSwap and compare\\n![slider-left](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-left.png&w=750&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n![slider-right](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-right.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n![slider-handle](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-handle.png&w=96&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\nPython under the hood\\n“Langflow has transformed our RAG application development, letting us focus more on creativity and less on complexity.”\\n![Jan Schummers](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_3.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\nJan Schummers\\nSr. Software Engineer, WinWeb\\n### Drag. Drop. Deploy.\\n###### Don't let boilerplate code slow you down. Visual state flows, reusable components, and rapid iteration for you. Focus on creating AI magic.\\n![Agent code and UI](https://www.langflow.org/_next/image?url=%2Fimages%2Fdrag-drop-deploy.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-agent.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n#### Limitless Control\\n###### Use Python to customize anything and everything\\n![Run, Share and Collaborate](https://www.langflow.org/_next/image?url=%2Fimages%2Frun-share-collab.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-collab](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-collab.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n#### Run, Share and Collaborate.\\n###### Choose from hundreds of pre-built flows and components\\n![Real-time Iteration](https://www.langflow.org/svgs/realtime-iteration.svg)![mobile-infra](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-infra.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n#### Agents at your service\\n###### Run a single or fleet of agents with access to all your components as tools\\n![Dialog API](https://www.langflow.org/_next/image?url=%2Fimages%2Fdialog-api.png&w=1920&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\n#### Flow as an API\\n###### Use a free, enterprise-grade cloud to deploy your app\\n## From Notebook to Production\\nGetting your AI in front of real users shouldn’t be a headache.\\npip install Sign Up\\n#### Deploy yourself or sign up for a free cloud account\\n#### Deploy and scale on an enterprise-grade, secure cloud platform\\n#### Same Langflow whether you’re using OSS or Cloud\\n### Connect your existing tools\\nChoose from hundreds of data sources, models, or vector stores. If don’t find what your looking for, build your own custom component.\\nAirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle CloudAirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle Cloud\\nGoogle DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIAGoogle DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIA\\nOllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstashOllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstash\\nVectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AIVectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AI\\n“Langflow has completely transformed the way we iterate and deploy AI workflows.”\\n![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\\nJonathan Blomgren\\nCOO, BetterUp\\n# Create your first flow\\nJoin thousands of developers accelerating their AI workflows. Start your first Langflow project now.\\nGet Started for FreeStar on GitHub\\n![gradiant](https://www.langflow.org/images/getStarted.png?w=16&q=75)\\n###### [62k](https://github.com/langflow-ai/langflow)\\n###### [12k](https://discord.com/invite/EqksyE2EX9)\\n###### [8k](https://x.com/langflow_ai)\\n###### [6k](https://www.youtube.com/@Langflow)\\n###### © 2024. All rights reserved\\n\", content_quality_score=0.5, summary=None, child_urls=['https://www.langflow.org/desktop', 'https://www.langflow.org/', 'http://docs.langflow.org/', 'https://github.com/langflow-ai/langflow', 'https://discord.com/invite/EqksyE2EX9', 'https://x.com/langflow_ai', 'https://www.youtube.com/@Langflow'])"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to filter the documents with quality threshold\n",
        "def filter_by_quality(\n",
        "    documents: list[Document],\n",
        "    content_quality_score_threshold: float,\n",
        ") -> Annotated[list[Document], \"filtered_documents\"]:\n",
        "    assert 0 <= content_quality_score_threshold <= 1, (\n",
        "        \"Content quality score threshold must be between 0 and 1\"\n",
        "    )\n",
        "\n",
        "    valid_docs = [\n",
        "        doc\n",
        "        for doc in documents\n",
        "        if not doc.content_quality_score\n",
        "        or doc.content_quality_score > content_quality_score_threshold\n",
        "    ]\n",
        "\n",
        "    return valid_docs"
      ],
      "metadata": {
        "id": "wsrdJEdYykuc"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define embedding models - either openai or Huggingface"
      ],
      "metadata": {
        "id": "Oe-9TpPIpieW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Any, Generator, Literal, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_core.documents import Document as LangChainDocument\n",
        "from langchain_mongodb.retrievers import (\n",
        "    MongoDBAtlasParentDocumentRetriever,\n",
        ")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "GIsUR0vuzHbo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EmbeddingModelType = Literal[\"openai\", \"huggingface\"]\n",
        "EmbeddingsModel = Union[OpenAIEmbeddings, HuggingFaceEmbeddings]\n",
        "\n",
        "# get openai embedding model\n",
        "def get_openai_embedding_model(model_id: str) -> OpenAIEmbeddings:\n",
        "    return OpenAIEmbeddings(\n",
        "        model=model_id,\n",
        "        allowed_special={\"<|endoftext|>\"},\n",
        "    )\n",
        "\n",
        "# get huggingface embedding model\n",
        "def get_huggingface_embedding_model(\n",
        "    model_id: str, device: str\n",
        ") -> HuggingFaceEmbeddings:\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=model_id,\n",
        "        model_kwargs={\"device\": device, \"trust_remote_code\": True},\n",
        "        encode_kwargs={\"normalize_embeddings\": False},\n",
        "    )\n",
        "\n",
        "# get embedding model\n",
        "def get_embedding_model(\n",
        "    model_id: str,\n",
        "    model_type: EmbeddingModelType = \"huggingface\",\n",
        "    device: str = \"cpu\",\n",
        ") -> EmbeddingsModel:\n",
        "\n",
        "    if model_type == \"openai\":\n",
        "        return get_openai_embedding_model(model_id)\n",
        "    elif model_type == \"huggingface\":\n",
        "        return get_huggingface_embedding_model(model_id, device)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid embedding model type: {model_type}\")"
      ],
      "metadata": {
        "id": "k8xy_Ld5ziHp"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual summary using OpenAI"
      ],
      "metadata": {
        "id": "RIGKi7tpVF19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hugging face api and endpoint\n",
        "HUGGINGFACE_DEDICATED_ENDPOINT = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n",
        "HUGGINGFACE_ACCESS_TOKEN = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "yylqz96piM4U"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "\n",
        "import psutil\n",
        "from litellm import acompletion, completion\n",
        "from loguru import logger\n",
        "from openai import AsyncOpenAI\n",
        "from pydantic import BaseModel\n",
        "from tqdm.asyncio import tqdm\n",
        "\n",
        "from typing import Callable, Literal, Union\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from loguru import logger"
      ],
      "metadata": {
        "id": "dAi_7IvJiRju"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define contextual document structure\n",
        "class ContextualDocument(BaseModel):\n",
        "    content: str\n",
        "    chunk: str | None = None\n",
        "    contextual_summarization: str | None = None\n",
        "\n",
        "    def add_contextual_summarization(self, summary: str) -> \"ContextualDocument\":\n",
        "        self.contextual_summarization = summary\n",
        "        return self"
      ],
      "metadata": {
        "id": "S-0lDEpiiUyZ"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk dataset and create contextual summary"
      ],
      "metadata": {
        "id": "8yM_-wmhVUZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document = valid_docs[0]\n",
        "chunk_size = 3072\n",
        "\n",
        "chunk_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                                                    encoding_name=\"cl100k_base\",\n",
        "                                                    chunk_size=chunk_size,\n",
        "                                                    chunk_overlap=int(0.10 * chunk_size),\n",
        "                                                )\n",
        "chunks = chunk_splitter.split_text(document.content)"
      ],
      "metadata": {
        "id": "RzIuUuDpnGRm"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2x105Hzjo8ac",
        "outputId": "2389bd0c-07f1-49bc-a030-735d1c79c087"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunks[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDY8P84Yo5by",
        "outputId": "d506c3c1-4160-4240-e8b1-eaa3c1297325"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### Langflow Desktop for macOS is here!\n",
            "[Try the alpha](https://www.langflow.org/desktop)\n",
            "[![Langflow Logo](https://www.langflow.org/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.f6f14a6e.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)](https://www.langflow.org/)\n",
            "###### [Docs](http://docs.langflow.org/)\n",
            "###### Resources\n",
            "###### Community\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "# Stop fighting your tools\n",
            "Langflow is a low-code builder that makes it easier to build powerful AIs that can use any API, model, or database.\n",
            "Get Started for FreeStar on GitHub\n",
            "Input\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "API Key\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "##### Used by leading AI development teams\n",
            "![gradient](https://www.langflow.org/images/Gradient.png?w=16&q=75)\n",
            "“Langflow lets us take complex product ideas and quickly bring them to life through visual flows that anyone can understand.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_2.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "Ditch the Black Boxes\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "Response Length\n",
            "Short\n",
            "Medium\n",
            "Long\n",
            "Control the complexity\n",
            "![Chat](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-chat.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Model](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-model.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-agent.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Swap and compare\n",
            "![slider-left](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-left.png&w=750&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-right](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-right.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-handle](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-handle.png&w=96&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Python under the hood\n",
            "“Langflow has transformed our RAG application development, letting us focus more on creativity and less on complexity.”\n",
            "![Jan Schummers](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_3.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jan Schummers\n",
            "Sr. Software Engineer, WinWeb\n",
            "### Drag. Drop. Deploy.\n",
            "###### Don't let boilerplate code slow you down. Visual state flows, reusable components, and rapid iteration for you. Focus on creating AI magic.\n",
            "![Agent code and UI](https://www.langflow.org/_next/image?url=%2Fimages%2Fdrag-drop-deploy.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-agent.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Limitless Control\n",
            "###### Use Python to customize anything and everything\n",
            "![Run, Share and Collaborate](https://www.langflow.org/_next/image?url=%2Fimages%2Frun-share-collab.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-collab](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-collab.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Run, Share and Collaborate.\n",
            "###### Choose from hundreds of pre-built flows and components\n",
            "![Real-time Iteration](https://www.langflow.org/svgs/realtime-iteration.svg)![mobile-infra](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-infra.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Agents at your service\n",
            "###### Run a single or fleet of agents with access to all your components as tools\n",
            "![Dialog API](https://www.langflow.org/_next/image?url=%2Fimages%2Fdialog-api.png&w=1920&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Flow as an API\n",
            "###### Use a free, enterprise-grade cloud to deploy your app\n",
            "## From Notebook to Production\n",
            "Getting your AI in front of real users shouldn’t be a headache.\n",
            "pip install Sign Up\n",
            "#### Deploy yourself or sign up for a free cloud account\n",
            "#### Deploy and scale on an enterprise-grade, secure cloud platform\n",
            "#### Same Langflow whether you’re using OSS or Cloud\n",
            "### Connect your existing tools\n",
            "Choose from hundreds of data sources, models, or vector stores. If don’t find what your looking for, build your own custom component.\n",
            "AirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle CloudAirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle Cloud\n",
            "Google DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIAGoogle DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIA\n",
            "OllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstashOllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstash\n",
            "VectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AIVectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AI\n",
            "“Langflow has completely transformed the way we iterate and deploy AI workflows.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "# Create your first flow\n",
            "Join thousands of developers accelerating their AI workflows. Start your first Langflow project now.\n",
            "Get Started for FreeStar on GitHub\n",
            "![gradiant](https://www.langflow.org/images/getStarted.png?w=16&q=75)\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "###### © 2024. All rights reserved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ls54YXe_o5ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [ContextualDocument(content=document.content, chunk=chunk) for chunk in chunks][0]\n",
        "\n",
        "print(documents.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJh1rnPMidDU",
        "outputId": "f903b087-5ab2-4f06-f376-bfaf37d4b603"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### Langflow Desktop for macOS is here!\n",
            "[Try the alpha](https://www.langflow.org/desktop)\n",
            "[![Langflow Logo](https://www.langflow.org/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.f6f14a6e.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)](https://www.langflow.org/)\n",
            "###### [Docs](http://docs.langflow.org/)\n",
            "###### Resources\n",
            "###### Community\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "# Stop fighting your tools\n",
            "Langflow is a low-code builder that makes it easier to build powerful AIs that can use any API, model, or database.\n",
            "Get Started for FreeStar on GitHub\n",
            "Input\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "API Key\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "##### Used by leading AI development teams\n",
            "![gradient](https://www.langflow.org/images/Gradient.png?w=16&q=75)\n",
            "“Langflow lets us take complex product ideas and quickly bring them to life through visual flows that anyone can understand.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_2.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "Ditch the Black Boxes\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "Response Length\n",
            "Short\n",
            "Medium\n",
            "Long\n",
            "Control the complexity\n",
            "![Chat](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-chat.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Model](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-model.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-agent.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Swap and compare\n",
            "![slider-left](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-left.png&w=750&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-right](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-right.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-handle](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-handle.png&w=96&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Python under the hood\n",
            "“Langflow has transformed our RAG application development, letting us focus more on creativity and less on complexity.”\n",
            "![Jan Schummers](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_3.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jan Schummers\n",
            "Sr. Software Engineer, WinWeb\n",
            "### Drag. Drop. Deploy.\n",
            "###### Don't let boilerplate code slow you down. Visual state flows, reusable components, and rapid iteration for you. Focus on creating AI magic.\n",
            "![Agent code and UI](https://www.langflow.org/_next/image?url=%2Fimages%2Fdrag-drop-deploy.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-agent.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Limitless Control\n",
            "###### Use Python to customize anything and everything\n",
            "![Run, Share and Collaborate](https://www.langflow.org/_next/image?url=%2Fimages%2Frun-share-collab.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-collab](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-collab.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Run, Share and Collaborate.\n",
            "###### Choose from hundreds of pre-built flows and components\n",
            "![Real-time Iteration](https://www.langflow.org/svgs/realtime-iteration.svg)![mobile-infra](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-infra.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Agents at your service\n",
            "###### Run a single or fleet of agents with access to all your components as tools\n",
            "![Dialog API](https://www.langflow.org/_next/image?url=%2Fimages%2Fdialog-api.png&w=1920&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Flow as an API\n",
            "###### Use a free, enterprise-grade cloud to deploy your app\n",
            "## From Notebook to Production\n",
            "Getting your AI in front of real users shouldn’t be a headache.\n",
            "pip install Sign Up\n",
            "#### Deploy yourself or sign up for a free cloud account\n",
            "#### Deploy and scale on an enterprise-grade, secure cloud platform\n",
            "#### Same Langflow whether you’re using OSS or Cloud\n",
            "### Connect your existing tools\n",
            "Choose from hundreds of data sources, models, or vector stores. If don’t find what your looking for, build your own custom component.\n",
            "AirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle CloudAirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle Cloud\n",
            "Google DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIAGoogle DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIA\n",
            "OllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstashOllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstash\n",
            "VectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AIVectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AI\n",
            "“Langflow has completely transformed the way we iterate and deploy AI workflows.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "# Create your first flow\n",
            "Join thousands of developers accelerating their AI workflows. Start your first Langflow project now.\n",
            "Get Started for FreeStar on GitHub\n",
            "![gradiant](https://www.langflow.org/images/getStarted.png?w=16&q=75)\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "###### © 2024. All rights reserved\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant specialized in summarizing documents relative to a given chunk.\n",
        "    <document>\n",
        "    {content}\n",
        "    </document>\n",
        "    Here is the chunk we want to situate within the whole document\n",
        "    <chunk>\n",
        "    {chunk}\n",
        "    </chunk>\n",
        "    Please give a short succinct context of maximum {characters} characters to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
        "    \"\"\""
      ],
      "metadata": {
        "id": "jrqf1HTLidF2"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(\n",
        "            model='gpt-4o-mini',\n",
        "            messages=[\n",
        "                    {\n",
        "                        \"role\": \"system\",\n",
        "                        \"content\": SYSTEM_PROMPT_TEMPLATE.format(\n",
        "                            characters=128,\n",
        "                            content=documents.content[\n",
        "                                :6000\n",
        "                            ],  # Keep it short to lower latency and costs.\n",
        "                            chunk=documents.chunk,\n",
        "                        ),\n",
        "                    },\n",
        "                ],\n",
        "                stream=False,\n",
        "                temperature=0,\n",
        "            )"
      ],
      "metadata": {
        "id": "t16oEK2pp3aP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFRE3Xm-pvOo",
        "outputId": "cebb2986-9c05-4da8-d6a9-564ecab09de6"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Langflow Desktop for macOS introduces a low-code AI builder for seamless integration and deployment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rkor1llgidAb"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a class to modify langchain RecursiveCharacterTextSplitter to handle summarisation post chunking.\n",
        "class HandlerRecursiveCharacterTextSplitter(RecursiveCharacterTextSplitter):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        handler: Callable[[str, list[str]], list[str]] | None = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.handler = handler if handler is not None else lambda _, x: x\n",
        "\n",
        "    def split_text(self, text: str) -> list[str]:\n",
        "        chunks = super().split_text(text)\n",
        "        parsed_chunks = self.handler(text, chunks)\n",
        "\n",
        "        return parsed_chunks"
      ],
      "metadata": {
        "id": "eiN4UC97nFhX"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# batch processing for contextual summary using openai\n",
        "class ContextualSummarizationAgent:\n",
        "\n",
        "    SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant specialized in summarizing documents relative to a given chunk.\n",
        "    <document>\n",
        "    {content}\n",
        "    </document>\n",
        "    Here is the chunk we want to situate within the whole document\n",
        "    <chunk>\n",
        "    {chunk}\n",
        "    </chunk>\n",
        "    Please give a short succinct context of maximum {characters} characters to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"gpt-4o-mini\",\n",
        "        max_characters: int = 128,\n",
        "        mock: bool = False,\n",
        "        max_concurrent_requests: int = 4,\n",
        "    ) -> None:\n",
        "        self.model_id = model_id\n",
        "        self.max_characters = max_characters\n",
        "        self.mock = mock\n",
        "        self.max_concurrent_requests = max_concurrent_requests\n",
        "\n",
        "    def __call__(self, content: str, chunks: list[str]) -> list[str]:\n",
        "        try:\n",
        "            loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            results = asyncio.run(self.__summarize_context_batch(content, chunks))\n",
        "        else:\n",
        "            results = loop.run_until_complete(\n",
        "                self.__summarize_context_batch(content, chunks)\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context_batch(\n",
        "        self, content: str, chunks: list[str]\n",
        "    ) -> list[str]:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        start_mem = process.memory_info().rss\n",
        "        total_chunks = len(chunks)\n",
        "\n",
        "        documents = [\n",
        "            ContextualDocument(content=content, chunk=chunk) for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        summarized_documents = await self.__process_batch(\n",
        "            documents, await_time_seconds=7\n",
        "        )\n",
        "        documents_with_summaries = [\n",
        "            doc\n",
        "            for doc in summarized_documents\n",
        "            if doc.contextual_summarization is not None\n",
        "        ]\n",
        "        documents_without_summaries = [\n",
        "            doc for doc in documents if doc.contextual_summarization is None\n",
        "        ]\n",
        "\n",
        "        # Retry failed documents with increased await time\n",
        "        if documents_without_summaries:\n",
        "            retry_results = await self.__process_batch(\n",
        "                documents_without_summaries, await_time_seconds=20\n",
        "            )\n",
        "            documents_with_summaries += retry_results\n",
        "\n",
        "        end_mem = process.memory_info().rss\n",
        "        memory_diff = end_mem - start_mem\n",
        "\n",
        "        success_count = len(documents_with_summaries)\n",
        "        failed_count = total_chunks - success_count\n",
        "\n",
        "        contextual_chunks = []\n",
        "        for doc in documents_with_summaries:\n",
        "            if doc.contextual_summarization is not None:\n",
        "                chunk = f\"{doc.contextual_summarization}\\n\\n{doc.chunk}\"\n",
        "            else:\n",
        "                chunk = f\"{doc.chunk}\"\n",
        "\n",
        "            contextual_chunks.append(chunk)\n",
        "\n",
        "        return contextual_chunks\n",
        "\n",
        "    async def __process_batch(\n",
        "        self, documents: list[ContextualDocument], await_time_seconds: int\n",
        "    ) -> list[ContextualDocument]:\n",
        "\n",
        "        semaphore = asyncio.Semaphore(self.max_concurrent_requests)\n",
        "        tasks = [\n",
        "            self.__summarize_context(\n",
        "                document, semaphore, await_time_seconds=await_time_seconds\n",
        "            )\n",
        "            for document in documents\n",
        "        ]\n",
        "        results = []\n",
        "        for coro in tqdm(\n",
        "            asyncio.as_completed(tasks),\n",
        "            total=len(documents),\n",
        "            desc=\"Processing documents\",\n",
        "            unit=\"doc\",\n",
        "        ):\n",
        "            result = await coro\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context(\n",
        "        self,\n",
        "        document: ContextualDocument,\n",
        "        semaphore: asyncio.Semaphore | None = None,\n",
        "        await_time_seconds: int = 2,\n",
        "    ) -> ContextualDocument:\n",
        "        if self.mock:\n",
        "            return document.add_contextual_summarization(\"This is a mock summary\")\n",
        "\n",
        "        async def process_document() -> ContextualDocument:\n",
        "            try:\n",
        "                response = await acompletion(\n",
        "                    model=self.model_id,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": self.SYSTEM_PROMPT_TEMPLATE.format(\n",
        "                                characters=self.max_characters,\n",
        "                                content=document.content[\n",
        "                                    :6000\n",
        "                                ],  # Keep it short to lower latency and costs.\n",
        "                                chunk=document.chunk,\n",
        "                            ),\n",
        "                        },\n",
        "                    ],\n",
        "                    stream=False,\n",
        "                    temperature=0,\n",
        "                )\n",
        "                await asyncio.sleep(await_time_seconds)  # Rate limiting\n",
        "\n",
        "                if not response.choices:\n",
        "                    return document\n",
        "\n",
        "                context_summary: str = response.choices[0].message.content\n",
        "                return document.add_contextual_summarization(context_summary)\n",
        "            except Exception as e:\n",
        "                return document\n",
        "\n",
        "        if semaphore:\n",
        "            async with semaphore:\n",
        "                return await process_document()\n",
        "\n",
        "        return await process_document()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kIp2qn5F0BpQ"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual summary using HuggingFace LLama-4"
      ],
      "metadata": {
        "id": "aVM5JW4hVOBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# batch processing for contextual summary using huggingface\n",
        "class SimpleSummarizationAgent:\n",
        "\n",
        "    SYSTEM_PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    You are a helpful assistant specialized in summarizing documents for the purposes of improving semantic and keyword search retrieval.\n",
        "    Generate a concise TL;DR summary in plain text format having a maximum of {characters} characters of the key findings from the provided documents,\n",
        "    highlighting the most significant insights. Answer only with the succinct context and nothing else.\n",
        "\n",
        "    ### Input:\n",
        "    {content}\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"gpt-4o-mini\",\n",
        "        base_url: str | None = HUGGINGFACE_DEDICATED_ENDPOINT,\n",
        "        api_key: str | None = HUGGINGFACE_ACCESS_TOKEN,\n",
        "        max_characters: int = 128,\n",
        "        mock: bool = False,\n",
        "        max_concurrent_requests: int = 4,\n",
        "    ) -> None:\n",
        "        self.model_id = model_id\n",
        "        self.base_url = base_url\n",
        "        self.api_key = api_key\n",
        "        self.max_characters = max_characters\n",
        "        self.mock = mock\n",
        "        self.max_concurrent_requests = max_concurrent_requests\n",
        "\n",
        "        if self.model_id == \"tgi\":\n",
        "            assert self.base_url and self.api_key, (\n",
        "                \"Base URL and API key are required for TGI Hugging Face Dedicated Endpoint\"\n",
        "            )\n",
        "\n",
        "            self.client = AsyncOpenAI(\n",
        "                base_url=self.base_url,\n",
        "                api_key=self.api_key,\n",
        "            )\n",
        "        else:\n",
        "            self.client = AsyncOpenAI()\n",
        "\n",
        "    def __call__(self, content: str, chunks: list[str]) -> list[str]:\n",
        "        try:\n",
        "            loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            results = asyncio.run(self.__summarize_context_batch(content, chunks))\n",
        "        else:\n",
        "            results = loop.run_until_complete(\n",
        "                self.__summarize_context_batch(content, chunks)\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context_batch(\n",
        "        self, content: str, chunks: list[str]\n",
        "    ) -> list[str]:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        start_mem = process.memory_info().rss\n",
        "        logger.debug(\n",
        "            f\"Starting summarizing document.\"\n",
        "            f\"Initial memory usage: {start_mem // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        document = await self.__summarize(\n",
        "            document=ContextualDocument(content=content), await_time_seconds=20\n",
        "        )\n",
        "\n",
        "        end_mem = process.memory_info().rss\n",
        "        memory_diff = end_mem - start_mem\n",
        "        logger.debug(\n",
        "            f\"Summarization completed. \"\n",
        "            f\"Final memory usage: {end_mem // (1024 * 1024)} MB, \"\n",
        "            f\"Memory difference: {memory_diff // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        contextual_chunks = []\n",
        "        for chunk in chunks:\n",
        "            if document.contextual_summarization is not None:\n",
        "                chunk = f\"{document.contextual_summarization}\\n\\n{chunk}\"\n",
        "            else:\n",
        "                chunk = f\"{chunk}\"\n",
        "\n",
        "            contextual_chunks.append(chunk)\n",
        "\n",
        "        return contextual_chunks\n",
        "\n",
        "    async def __summarize(\n",
        "        self,\n",
        "        document: ContextualDocument,\n",
        "        await_time_seconds: int = 2,\n",
        "    ) -> ContextualDocument:\n",
        "        if self.mock:\n",
        "            return document.add_contextual_summarization(\"This is a mock summary\")\n",
        "\n",
        "        async def process_document() -> ContextualDocument:\n",
        "            try:\n",
        "                response = await self.client.chat.completions.create(\n",
        "                    model=self.model_id,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": self.SYSTEM_PROMPT_TEMPLATE.format(\n",
        "                                characters=self.max_characters, content=document.content\n",
        "                            ),\n",
        "                        },\n",
        "                    ],\n",
        "                    stream=False,\n",
        "                    temperature=0,\n",
        "                )\n",
        "                await asyncio.sleep(await_time_seconds)  # Rate limiting\n",
        "\n",
        "                if not response.choices:\n",
        "                    logger.warning(\"No contextual summary generated for chunk\")\n",
        "                    return document\n",
        "\n",
        "                context_summary: str = response.choices[0].message.content\n",
        "                return document.add_contextual_summarization(context_summary)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to generate contextual summary: {str(e)}\")\n",
        "                return document\n",
        "\n",
        "        return await process_document()"
      ],
      "metadata": {
        "id": "1oWi2dz8UO2g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add type definitions at the top of the file\n",
        "SummarizationType = Literal[\"contextual\", \"simple\", \"none\"]\n",
        "SummarizationAgent = Union[ContextualSummarizationAgent, SimpleSummarizationAgent]\n",
        "\n",
        "# get the chunk splitter based on the summarization_type\n",
        "def get_splitter(\n",
        "    chunk_size: int, summarization_type: SummarizationType = \"none\", **kwargs\n",
        ") -> RecursiveCharacterTextSplitter:\n",
        "    chunk_overlap = int(0.15 * chunk_size)\n",
        "\n",
        "    if summarization_type == \"none\":\n",
        "        return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            encoding_name=\"cl100k_base\",\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "        )\n",
        "\n",
        "    if summarization_type == \"contextual\":\n",
        "        handler = ContextualSummarizationAgent(**kwargs)\n",
        "    elif summarization_type == \"simple\":\n",
        "        handler = SimpleSummarizationAgent(**kwargs)\n",
        "\n",
        "    return HandlerRecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        encoding_name=\"cl100k_base\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        handler=handler,\n",
        "    )"
      ],
      "metadata": {
        "id": "X1yRD_Z7zy0l"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = get_splitter(chunk_size, \"none\")"
      ],
      "metadata": {
        "id": "IHVYtgAvsWar"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunk = splitter.split_text(document.content)"
      ],
      "metadata": {
        "id": "N4QEFLhisrer"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chunk[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2GLQYrbUtHhs",
        "outputId": "4c6dce5d-2f24-48a0-aa81-c99121a603e7"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###### Langflow Desktop for macOS is here!\n",
            "[Try the alpha](https://www.langflow.org/desktop)\n",
            "[![Langflow Logo](https://www.langflow.org/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Flogo.f6f14a6e.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)](https://www.langflow.org/)\n",
            "###### [Docs](http://docs.langflow.org/)\n",
            "###### Resources\n",
            "###### Community\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "# Stop fighting your tools\n",
            "Langflow is a low-code builder that makes it easier to build powerful AIs that can use any API, model, or database.\n",
            "Get Started for FreeStar on GitHub\n",
            "Input\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "API Key\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "##### Used by leading AI development teams\n",
            "![gradient](https://www.langflow.org/images/Gradient.png?w=16&q=75)\n",
            "“Langflow lets us take complex product ideas and quickly bring them to life through visual flows that anyone can understand.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_2.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "Ditch the Black Boxes\n",
            "Model\n",
            "l3\n",
            "llama-3.2\n",
            "Temperature\n",
            "0.5\n",
            "Precise\n",
            "Creative\n",
            "Response Length\n",
            "Short\n",
            "Medium\n",
            "Long\n",
            "Control the complexity\n",
            "![Chat](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-chat.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Model](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-model.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![Agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fcontent-agent.png&w=640&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Swap and compare\n",
            "![slider-left](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-left.png&w=750&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-right](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-right.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "![slider-handle](https://www.langflow.org/_next/image?url=%2Fimages%2Fslider-handle.png&w=96&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Python under the hood\n",
            "“Langflow has transformed our RAG application development, letting us focus more on creativity and less on complexity.”\n",
            "![Jan Schummers](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo_3.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jan Schummers\n",
            "Sr. Software Engineer, WinWeb\n",
            "### Drag. Drop. Deploy.\n",
            "###### Don't let boilerplate code slow you down. Visual state flows, reusable components, and rapid iteration for you. Focus on creating AI magic.\n",
            "![Agent code and UI](https://www.langflow.org/_next/image?url=%2Fimages%2Fdrag-drop-deploy.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-agent](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-agent.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Limitless Control\n",
            "###### Use Python to customize anything and everything\n",
            "![Run, Share and Collaborate](https://www.langflow.org/_next/image?url=%2Fimages%2Frun-share-collab.png&w=2048&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)![mobile-collab](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-collab.png&w=1200&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Run, Share and Collaborate.\n",
            "###### Choose from hundreds of pre-built flows and components\n",
            "![Real-time Iteration](https://www.langflow.org/svgs/realtime-iteration.svg)![mobile-infra](https://www.langflow.org/_next/image?url=%2Fimages%2Fmobile-infra.png&w=3840&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Agents at your service\n",
            "###### Run a single or fleet of agents with access to all your components as tools\n",
            "![Dialog API](https://www.langflow.org/_next/image?url=%2Fimages%2Fdialog-api.png&w=1920&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "#### Flow as an API\n",
            "###### Use a free, enterprise-grade cloud to deploy your app\n",
            "## From Notebook to Production\n",
            "Getting your AI in front of real users shouldn’t be a headache.\n",
            "pip install Sign Up\n",
            "#### Deploy yourself or sign up for a free cloud account\n",
            "#### Deploy and scale on an enterprise-grade, secure cloud platform\n",
            "#### Same Langflow whether you’re using OSS or Cloud\n",
            "### Connect your existing tools\n",
            "Choose from hundreds of data sources, models, or vector stores. If don’t find what your looking for, build your own custom component.\n",
            "AirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle CloudAirbyteAnthropicAzureBingComposioConfluenceCouchbaseEvernoteGithubGleanGmailGoogle Cloud\n",
            "Google DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIAGoogle DriveGroqSupaHuggingFaceLangchainMetaReddisMilvusMistralMongoDBNotionNVIDIA\n",
            "OllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstashOllamaPerplexityWeaviateSerperQdrantSerp APISlackSupaDatastaxAmazon BedrockTavilyUpstash\n",
            "VectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AIVectaraWikipediaWolfram AlphaYahoo! FinanceZapMeriTalkPineconeCassandraUnstructuredOpenAICrew AI\n",
            "“Langflow has completely transformed the way we iterate and deploy AI workflows.”\n",
            "![Jonathan Blomgren](https://www.langflow.org/_next/image?url=%2Fimages%2FCoo.png&w=256&q=75&dpl=dpl_3yYP3T4nMYAPkC9GH9Eg5Aqo1rx7)\n",
            "Jonathan Blomgren\n",
            "COO, BetterUp\n",
            "# Create your first flow\n",
            "Join thousands of developers accelerating their AI workflows. Start your first Langflow project now.\n",
            "Get Started for FreeStar on GitHub\n",
            "![gradiant](https://www.langflow.org/images/getStarted.png?w=16&q=75)\n",
            "###### [62k](https://github.com/langflow-ai/langflow)\n",
            "###### [12k](https://discord.com/invite/EqksyE2EX9)\n",
            "###### [8k](https://x.com/langflow_ai)\n",
            "###### [6k](https://www.youtube.com/@Langflow)\n",
            "###### © 2024. All rights reserved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Retrievers"
      ],
      "metadata": {
        "id": "WDGBV2g8VzQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal, Union\n",
        "\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain_mongodb.retrievers import (\n",
        "    MongoDBAtlasHybridSearchRetriever,\n",
        "    MongoDBAtlasParentDocumentRetriever,\n",
        ")\n",
        "\n",
        "# Add these type definitions at the top of the file\n",
        "RetrieverType = Literal[\"contextual\", \"parent\"]\n",
        "RetrieverModel = Union[\n",
        "    MongoDBAtlasHybridSearchRetriever, MongoDBAtlasParentDocumentRetriever\n",
        "]\n",
        "\n",
        "def get_hybrid_search_retriever(\n",
        "    embedding_model: EmbeddingsModel, k: int\n",
        ") -> MongoDBAtlasHybridSearchRetriever:\n",
        "    vectorstore = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "        connection_string=MONGODB_URI,\n",
        "        embedding=embedding_model,\n",
        "        namespace=f\"{MONGODB_DATABASE_NAME}.rag\",\n",
        "        text_key=\"chunk\",\n",
        "        embedding_key=\"embedding\",\n",
        "        relevance_score_fn=\"dotProduct\",\n",
        "    )\n",
        "\n",
        "    retriever = MongoDBAtlasHybridSearchRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        search_index_name=\"chunk_text_search\",\n",
        "        top_k=k,\n",
        "        vector_penalty=50,\n",
        "        fulltext_penalty=50,\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def get_parent_document_retriever(\n",
        "    embedding_model: EmbeddingsModel, k: int = 3\n",
        ") -> MongoDBAtlasParentDocumentRetriever:\n",
        "    retriever = MongoDBAtlasParentDocumentRetriever.from_connection_string(\n",
        "        connection_string=MONGODB_URI,\n",
        "        embedding_model=embedding_model,\n",
        "        child_splitter=get_splitter(200),\n",
        "        parent_splitter=get_splitter(800),\n",
        "        database_name=MONGODB_DATABASE_NAME,\n",
        "        collection_name=\"rag\",\n",
        "        text_key=\"page_content\",\n",
        "        search_kwargs={\"k\": k},\n",
        "    )\n",
        "\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "h2VWJGebzYI0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retriever(\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType = \"huggingface\",\n",
        "    retriever_type: RetrieverType = \"contextual\",\n",
        "    k: int = 3,\n",
        "    device: str = \"cpu\",\n",
        ") -> RetrieverModel:\n",
        "    embedding_model = get_embedding_model(\n",
        "        embedding_model_id, embedding_model_type, device\n",
        "    )\n",
        "\n",
        "    if retriever_type == \"contextual\":\n",
        "        return get_hybrid_search_retriever(embedding_model, k)\n",
        "    elif retriever_type == \"parent\":\n",
        "        return get_parent_document_retriever(embedding_model, k)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid retriever type: {retriever_type}\")"
      ],
      "metadata": {
        "id": "_Dvd2u61t9DE"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(\n",
        "    docs: list[LangChainDocument], batch_size: int\n",
        ") -> Generator[list[LangChainDocument], None, None]:\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        yield docs[i : i + batch_size]"
      ],
      "metadata": {
        "id": "mU8p2Gbbtq9g"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(\n",
        "    retriever: Any,\n",
        "    batch: list[LangChainDocument],\n",
        "    splitter: RecursiveCharacterTextSplitter,\n",
        ") -> None:\n",
        "    try:\n",
        "        if isinstance(retriever, MongoDBAtlasParentDocumentRetriever):\n",
        "            retriever.add_documents(batch)\n",
        "        else:\n",
        "            split_docs = splitter.split_documents(batch)\n",
        "            retriever.vectorstore.add_documents(split_docs)\n",
        "    except Exception as e:\n",
        "        raise"
      ],
      "metadata": {
        "id": "Bt1-Wx5xtxG_"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_docs(\n",
        "    retriever: Any,\n",
        "    docs: list[LangChainDocument],\n",
        "    splitter: RecursiveCharacterTextSplitter,\n",
        "    batch_size: int = 4,\n",
        "    max_workers: int = 2,\n",
        ") -> list[None]:\n",
        "    batches = list(get_batches(docs, batch_size))\n",
        "    results = []\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [\n",
        "            executor.submit(process_batch, retriever, batch, splitter)\n",
        "            for batch in batches\n",
        "        ]\n",
        "\n",
        "        with tqdm(total=total_docs, desc=\"Processing documents\") as pbar:\n",
        "            for future in as_completed(futures):\n",
        "                result = future.result()\n",
        "                results.append(result)\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "7Ap5byvLzCVg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Index"
      ],
      "metadata": {
        "id": "Swu8bNSezhSb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mongodb.index import create_fulltext_search_index\n",
        "\n",
        "class MongoDBIndex:\n",
        "    def __init__(\n",
        "        self,\n",
        "        retriever,\n",
        "        mongodb_client: MongoDBService,\n",
        "    ) -> None:\n",
        "        self.retriever = retriever\n",
        "        self.mongodb_client = mongodb_client\n",
        "\n",
        "    def create(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        is_hybrid: bool = False,\n",
        "    ) -> None:\n",
        "        vectorstore = self.retriever.vectorstore\n",
        "\n",
        "        vectorstore.create_vector_search_index(\n",
        "            dimensions=embedding_dim,\n",
        "        )\n",
        "        if is_hybrid:\n",
        "            create_fulltext_search_index(\n",
        "                collection=self.mongodb_client.collection,\n",
        "                field=vectorstore._text_key,\n",
        "                index_name=self.retriever.search_index_name,\n",
        "            )"
      ],
      "metadata": {
        "id": "Lo-YmI1ZYzA-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "r97hwOLh07tf"
      },
      "outputs": [],
      "source": [
        "def chunk_embed_load(\n",
        "    documents: list[Document],\n",
        "    collection_name: str,\n",
        "    processing_batch_size: int,\n",
        "    processing_max_workers: int,\n",
        "    retriever_type: RetrieverType,\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType,\n",
        "    embedding_model_dim: int,\n",
        "    chunk_size: int,\n",
        "    contextual_summarization_type: SummarizationType = \"none\",\n",
        "    contextual_agent_model_id: str | None = None,\n",
        "    contextual_agent_max_characters: int | None = None,\n",
        "    mock: bool = False,\n",
        "    device: str = \"cpu\",\n",
        ") -> None:\n",
        "\n",
        "    retriever = get_retriever(\n",
        "        embedding_model_id=embedding_model_id,\n",
        "        embedding_model_type=embedding_model_type,\n",
        "        retriever_type=retriever_type,\n",
        "        device=device,\n",
        "    )\n",
        "    splitter = get_splitter(\n",
        "        chunk_size=chunk_size,\n",
        "        summarization_type=contextual_summarization_type,\n",
        "        model_id=contextual_agent_model_id,\n",
        "        max_characters=contextual_agent_max_characters,\n",
        "        mock=mock,\n",
        "        max_concurrent_requests=processing_max_workers,\n",
        "    )\n",
        "\n",
        "    with MongoDBService(\n",
        "        model=Document, collection_name=collection_name\n",
        "    ) as mongodb_client:\n",
        "        mongodb_client.clear_collection()\n",
        "\n",
        "        docs = [\n",
        "            LangChainDocument(\n",
        "                page_content=doc.content, metadata=doc.metadata.model_dump()\n",
        "            )\n",
        "            for doc in documents\n",
        "            if doc\n",
        "        ]\n",
        "        process_docs(\n",
        "            retriever,\n",
        "            docs,\n",
        "            splitter=splitter,\n",
        "            batch_size=processing_batch_size,\n",
        "            max_workers=processing_max_workers,\n",
        "        )\n",
        "\n",
        "        index = MongoDBIndex(\n",
        "            retriever=retriever,\n",
        "            mongodb_client=mongodb_client,\n",
        "        )\n",
        "        index.create(\n",
        "            embedding_dim=embedding_model_dim,\n",
        "            is_hybrid=retriever_type == \"contextual\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CYeNem9l10wG"
      },
      "outputs": [],
      "source": [
        "extract_collection_name = COLLECTION_NAME\n",
        "fetch_limit = 30\n",
        "load_collection_name = 'rag'\n",
        "content_quality_score_threshold = 0.6\n",
        "retriever_type = 'contextual'\n",
        "embedding_model_id  = 'text-embedding-3-small'\n",
        "embedding_model_type = 'openai'\n",
        "embedding_model_dim = 1536\n",
        "chunk_size = 3072\n",
        "contextual_summarization_type = 'contextual'\n",
        "contextual_agent_model_id = 'gpt-4o-mini'\n",
        "contextual_agent_max_characters = 128\n",
        "mock = False\n",
        "processing_batch_size = 2\n",
        "processing_max_workers = 2\n",
        "device = 'cpu' # or cuda (for Nvidia GPUs) or mps (for Apple M1/M2/M3 chips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NZjBHhqFDb3t"
      },
      "outputs": [],
      "source": [
        "def compute_rag_vector_index(\n",
        "    extract_collection_name: str,\n",
        "    fetch_limit: int,\n",
        "    load_collection_name: str,\n",
        "    content_quality_score_threshold: float,\n",
        "    retriever_type: RetrieverType,\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType,\n",
        "    embedding_model_dim: int,\n",
        "    chunk_size: int,\n",
        "    contextual_summarization_type: SummarizationType = \"none\",\n",
        "    contextual_agent_model_id: str | None = None,\n",
        "    contextual_agent_max_characters: int | None = None,\n",
        "    mock: bool = False,\n",
        "    processing_batch_size: int = 256,\n",
        "    processing_max_workers: int = 10,\n",
        "    device: str = \"cpu\",\n",
        ") -> None:\n",
        "    documents = fetch_from_mongodb(\n",
        "        collection_name=extract_collection_name, limit=fetch_limit\n",
        "    )\n",
        "\n",
        "    documents = documents[:8]\n",
        "\n",
        "    documents = filter_by_quality(\n",
        "        documents=documents,\n",
        "        content_quality_score_threshold=content_quality_score_threshold,\n",
        "    )\n",
        "    chunk_embed_load(\n",
        "        documents=documents,\n",
        "        collection_name=load_collection_name,\n",
        "        processing_batch_size=processing_batch_size,\n",
        "        processing_max_workers=processing_max_workers,\n",
        "        retriever_type=retriever_type,\n",
        "        embedding_model_id=embedding_model_id,\n",
        "        embedding_model_type=embedding_model_type,\n",
        "        embedding_model_dim=embedding_model_dim,\n",
        "        chunk_size=chunk_size,\n",
        "        contextual_summarization_type=contextual_summarization_type,\n",
        "        contextual_agent_model_id=contextual_agent_model_id,\n",
        "        contextual_agent_max_characters=contextual_agent_max_characters,\n",
        "        mock=mock,\n",
        "        device=device,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_rag_vector_index(extract_collection_name = COLLECTION_NAME,\n",
        "                        fetch_limit = 30,\n",
        "                        load_collection_name = 'rag',\n",
        "                        content_quality_score_threshold = 0.1,\n",
        "                        retriever_type = 'contextual',\n",
        "                        embedding_model_id  = 'text-embedding-3-small',\n",
        "                        embedding_model_type = 'openai',\n",
        "                        embedding_model_dim = 1536,\n",
        "                        chunk_size = 3072,\n",
        "                        contextual_summarization_type = 'contextual',\n",
        "                        contextual_agent_model_id = 'gpt-4o-mini',\n",
        "                        contextual_agent_max_characters = 128,\n",
        "                        mock = False,\n",
        "                        processing_batch_size = 2,\n",
        "                        processing_max_workers = 2,\n",
        "                        device = 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05BT2xCzXsoJ",
        "outputId": "383ed664-862a-4388-9b8e-4fe5a2903a61"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-06-06 08:38:49.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGetting splitter with chunk size: 3072 and overlap: 460\u001b[0m\n",
            "Processing documents:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[32m2025-06-06 08:38:50.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 428 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\u001b[32m2025-06-06 08:38:50.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 3 chunks with 2 concurrent requests. Initial memory usage: 428 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/3 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.83s/doc]\n",
            "\u001b[32m2025-06-06 08:38:58.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 430 MB, Memory difference: 2 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:38:58.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:38:58.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 2 chunks with 2 concurrent requests. Initial memory usage: 430 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/2 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  33%|███▎      | 1/3 [00:08<00:17,  8.63s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  67%|██████▋   | 2/3 [00:11<00:04,  4.99s/doc]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 2/2 [00:07<00:00,  3.94s/doc]\n",
            "\u001b[32m2025-06-06 08:39:06.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 431 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:06.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 2/2 chunks summarized successfully ✓ | 0/2 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents: 100%|██████████| 3/3 [00:16<00:00,  5.66s/doc]\n",
            "\u001b[32m2025-06-06 08:39:07.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 431 MB, Memory difference: 3 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:07.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 3/3 chunks summarized successfully ✓ | 0/3 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:07.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 431 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\u001b[32m2025-06-06 08:39:08.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  25%|██▌       | 2/8 [00:17<00:53,  8.95s/it]\u001b[32m2025-06-06 08:39:08.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 7 chunks with 2 concurrent requests. Initial memory usage: 432 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/7 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.86s/doc]\n",
            "\u001b[32m2025-06-06 08:39:15.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 432 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:15.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  14%|█▍        | 1/7 [00:08<00:48,  8.10s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:16.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  50%|█████     | 4/8 [00:26<00:24,  6.10s/it]\u001b[32m2025-06-06 08:39:16.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 433 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  29%|██▊       | 2/7 [00:10<00:23,  4.63s/doc]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.60s/doc]\n",
            "\u001b[32m2025-06-06 08:39:24.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 433 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:24.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  43%|████▎     | 3/7 [00:15<00:20,  5.03s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:24.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 6 chunks with 2 concurrent requests. Initial memory usage: 433 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/6 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  57%|█████▋    | 4/7 [00:17<00:11,  3.89s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  71%|███████▏  | 5/7 [00:23<00:08,  4.46s/doc]\u001b[A\u001b[A\n",
            "Processing documents:  17%|█▋        | 1/6 [00:07<00:38,  7.73s/doc]\u001b[A\n",
            "\n",
            "Processing documents:  86%|████████▌ | 6/7 [00:25<00:03,  3.65s/doc]\u001b[A\u001b[A\n",
            "Processing documents:  50%|█████     | 3/6 [00:15<00:14,  4.76s/doc]\u001b[A\n",
            "\n",
            "Processing documents: 100%|██████████| 7/7 [00:31<00:00,  4.48s/doc]\n",
            "\u001b[32m2025-06-06 08:39:40.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 433 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:40.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 7/7 chunks summarized successfully ✓ | 0/7 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:40.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 8 chunks with 2 concurrent requests. Initial memory usage: 434 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/8 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents:  83%|████████▎ | 5/6 [00:22<00:04,  4.28s/doc]\u001b[A\n",
            "Processing documents: 100%|██████████| 6/6 [00:22<00:00,  3.83s/doc]\n",
            "\u001b[32m2025-06-06 08:39:47.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 434 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:47.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 6/6 chunks summarized successfully ✓ | 0/6 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  12%|█▎        | 1/8 [00:07<00:53,  7.61s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:48.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  75%|███████▌  | 6/8 [00:58<00:21, 10.60s/it]\n",
            "\n",
            "Processing documents:  38%|███▊      | 3/8 [00:15<00:24,  4.83s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  62%|██████▎   | 5/8 [00:22<00:12,  4.28s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents: 100%|██████████| 8/8 [00:30<00:00,  3.81s/doc]\n",
            "\u001b[32m2025-06-06 08:40:10.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 435 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:40:10.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 8/8 chunks summarized successfully ✓ | 0/8 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:40:11.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents: 100%|██████████| 8/8 [01:21<00:00, 10.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuzaWbobYAFl"
      },
      "execution_count": 55,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TELD2WAVersnpXQaDCGzbP3UGZkOzicS",
      "authorship_tag": "ABX9TyP89Wunh2OH2wAsDRxGm2f2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}