{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhardwaj-Saurabh/personal_knowledge_assistant/blob/master/rag_application_data_ingestion_Vector_Index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "xuRbkfk_qGX5"
      },
      "outputs": [],
      "source": [
        "# !pip install -r /content/drive/MyDrive/FInananceRagAgent/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !playwright install"
      ],
      "metadata": {
        "id": "jruPoCnQxTtQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9BjvGVHjqSa1"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Annotated\n",
        "import json\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel, Field\n",
        "import os\n",
        "\n",
        "from typing_extensions import Annotated\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "sbRW5kuO1sR6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "zTdJQoNkrPPJ"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "MONGODB_URI = userdata.get('MONGODB_URI')\n",
        "HUGGINGFACE_ACCESS_TOKEN = userdata.get('HF_TOKEN')\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "boEajKb-3Win"
      },
      "outputs": [],
      "source": [
        "class DocumentMetadata(BaseModel):\n",
        "    id: str\n",
        "    url: str\n",
        "    title: str\n",
        "    properties: dict\n",
        "\n",
        "    def obfuscate(self) -> \"DocumentMetadata\":\n",
        "        original_id = self.id.replace(\"-\", \"\")\n",
        "        fake_id = generate_random_hex(len(original_id))\n",
        "\n",
        "        self.id = fake_id\n",
        "        self.url = self.url.replace(original_id, fake_id)\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "v3b__XGS3Wio"
      },
      "outputs": [],
      "source": [
        "def generate_random_hex(length: int) -> str:\n",
        "    hex_chars = string.hexdigits.lower()\n",
        "    return \"\".join(random.choice(hex_chars) for _ in range(length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yoqH-sbu3Wip"
      },
      "outputs": [],
      "source": [
        "class Document(BaseModel):\n",
        "    id: str = Field(default_factory=lambda: generate_random_hex(length=32))\n",
        "    metadata: DocumentMetadata\n",
        "    parent_metadata: DocumentMetadata | None = None\n",
        "    content: str\n",
        "    content_quality_score: float | None = None\n",
        "    summary: str | None = None\n",
        "    child_urls: list[str] = Field(default_factory=list)\n",
        "\n",
        "    @classmethod\n",
        "    def from_file(cls, file_path: Path) -> \"Document\":\n",
        "        json_data = file_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "        return cls.model_validate_json(json_data)\n",
        "\n",
        "    def add_summary(self, summary: str) -> \"Document\":\n",
        "        self.summary = summary\n",
        "\n",
        "        return self\n",
        "\n",
        "    def add_quality_score(self, score: float) -> \"Document\":\n",
        "        self.content_quality_score = score\n",
        "\n",
        "        return self\n",
        "\n",
        "    def write(\n",
        "        self, output_dir: Path, obfuscate: bool = False, also_save_as_txt: bool = False\n",
        "    ) -> None:\n",
        "\n",
        "        output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        if obfuscate:\n",
        "            self.obfuscate()\n",
        "\n",
        "        json_page = self.model_dump()\n",
        "\n",
        "        output_file = output_dir / f\"{self.id}.json\"\n",
        "        with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(\n",
        "                json_page,\n",
        "                f,\n",
        "                indent=4,\n",
        "                ensure_ascii=False,\n",
        "            )\n",
        "\n",
        "        if also_save_as_txt:\n",
        "            txt_path = output_file.with_suffix(\".txt\")\n",
        "            with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(self.content)\n",
        "\n",
        "    def obfuscate(self) -> \"Document\":\n",
        "        self.metadata = self.metadata.obfuscate()\n",
        "        self.parent_metadata = (\n",
        "            self.parent_metadata.obfuscate() if self.parent_metadata else None\n",
        "        )\n",
        "        self.id = self.metadata.id\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __eq__(self, other: object) -> bool:\n",
        "        if not isinstance(other, Document):\n",
        "            return False\n",
        "        return self.id == other.id\n",
        "\n",
        "    def __hash__(self) -> int:\n",
        "        return hash(self.id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "R6h4Ly7I3Wip"
      },
      "outputs": [],
      "source": [
        "def __get_json_files(data_directory: Path, nesting_level: int = 0) -> list[Path]:\n",
        "    if nesting_level == 0:\n",
        "        return list(data_directory.glob(\"*.json\"))\n",
        "    else:\n",
        "        json_files = []\n",
        "        for database_dir in data_directory.iterdir():\n",
        "            if database_dir.is_dir():\n",
        "                nested_json_files = __get_json_files(\n",
        "                    data_directory=database_dir, nesting_level=nesting_level - 1\n",
        "                )\n",
        "                json_files.extend(nested_json_files)\n",
        "\n",
        "        return json_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pFKdqt66qrom"
      },
      "outputs": [],
      "source": [
        "COLLECTION_NAME = 'knowledge_assistant'\n",
        "MONGODB_DATABASE_NAME = 'knowledgeassistantdatabase'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "c6O1ln-jq_k0"
      },
      "outputs": [],
      "source": [
        "from typing import Generic, Type, TypeVar\n",
        "\n",
        "from bson import ObjectId\n",
        "from loguru import logger\n",
        "from pydantic import BaseModel\n",
        "from pymongo import MongoClient, errors\n",
        "\n",
        "T = TypeVar(\"T\", bound=BaseModel)\n",
        "\n",
        "\n",
        "class MongoDBService(Generic[T]):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Type[T],\n",
        "        collection_name: str = COLLECTION_NAME,\n",
        "        database_name: str = MONGODB_DATABASE_NAME,\n",
        "        mongodb_uri: str = MONGODB_URI,\n",
        "    ) -> None:\n",
        "        self.model = model\n",
        "        self.collection_name = collection_name\n",
        "        self.database_name = database_name\n",
        "        self.mongodb_uri = mongodb_uri\n",
        "\n",
        "        try:\n",
        "            self.client = MongoClient(mongodb_uri)\n",
        "            self.client.admin.command(\"ping\")\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "        self.database = self.client[database_name]\n",
        "        self.collection = self.database[collection_name]\n",
        "\n",
        "    def __enter__(self) -> \"MongoDBService\":\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n",
        "        self.close()\n",
        "\n",
        "    def clear_collection(self) -> None:\n",
        "        try:\n",
        "            result = self.collection.delete_many({})\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def ingest_documents(self, documents: list[T]) -> None:\n",
        "        try:\n",
        "            if not documents or not all(\n",
        "                isinstance(doc, BaseModel) for doc in documents\n",
        "            ):\n",
        "                raise ValueError(\"Documents must be a list of Pycantic models.\")\n",
        "\n",
        "            dict_documents = [doc.model_dump() for doc in documents]\n",
        "\n",
        "            # Remove '_id' fields to avoid duplicate key errors\n",
        "            for doc in dict_documents:\n",
        "                doc.pop(\"_id\", None)\n",
        "\n",
        "            self.collection.insert_many(dict_documents)\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def fetch_documents(self, limit: int, query: dict) -> list[T]:\n",
        "        try:\n",
        "            documents = list(self.collection.find(query).limit(limit))\n",
        "            return self.__parse_documents(documents)\n",
        "        except Exception as e:\n",
        "            raise\n",
        "\n",
        "    def __parse_documents(self, documents: list[dict]) -> list[T]:\n",
        "        parsed_documents = []\n",
        "        for doc in documents:\n",
        "            for key, value in doc.items():\n",
        "                if isinstance(value, ObjectId):\n",
        "                    doc[key] = str(value)\n",
        "\n",
        "            _id = doc.pop(\"_id\", None)\n",
        "            doc[\"id\"] = _id\n",
        "\n",
        "            parsed_doc = self.model.model_validate(doc)\n",
        "            parsed_documents.append(parsed_doc)\n",
        "\n",
        "        return parsed_documents\n",
        "\n",
        "    def get_collection_count(self) -> int:\n",
        "        try:\n",
        "            return self.collection.count_documents({})\n",
        "        except errors.PyMongoError as e:\n",
        "            raise\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.client.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fatch Data from MongoDB"
      ],
      "metadata": {
        "id": "DB49jhmwpG0_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CEYT0dfrqSYQ"
      },
      "outputs": [],
      "source": [
        "def fetch_from_mongodb(\n",
        "    collection_name: str,\n",
        "    limit: int,\n",
        ") -> Annotated[list[dict], \"documents\"]:\n",
        "    with MongoDBService(model=Document, collection_name=collection_name) as service:\n",
        "        documents = service.fetch_documents(limit, query={})\n",
        "\n",
        "    return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-JPFqsePqSVb"
      },
      "outputs": [],
      "source": [
        "documents = fetch_from_mongodb(\n",
        "    collection_name = COLLECTION_NAME,\n",
        "    limit = 1000,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "r6wqm7yGqSRR",
        "outputId": "69c735be-b927-43e1-ac74-90b04755b372"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "[Lightning AI Studios: Never set up a local environment again →](https://lightning.ai)\n**\nLog in or create a free Lightning.ai account to track your progress and access additional course materials[Get Started →](https://lightning.ai/sign-in?redirectTo=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F)\n**\nDeep Learning Fundamentals \n* Pages\n  * [Deep Learning Fundamentals](https://lightning.ai/courses/deep-learning-fundamentals/)\n    * [Unit 1Intro to ML and DL](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/)\n      * [Unit 1.1What Is ML?](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n      * [Unit 1.2How Can We Use ML?](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-2/)\n      * [Unit 1.3A Typical ML Workflow](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/unit-1-3/)\n      * [Unit 1.4The First ML Classifier](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/ml-classifier/)\n      * [Unit 1.5Setting Up Our Computing Environment](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-5-computing-environment/)\n      * [Unit 1.6Implementing a Perceptron in Python](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-6-implementing-a-perceptron-in-python-parts-1-3/)\n      * [Unit 1.7 Evaluating Machine Learning Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/1-7-evaluating-machine-learning-models-parts-1-and-2/)\n      * [Unit 1 ExercisesUnit 1 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-1/exercises/)\n    * [Unit 2Using Tensors w/ PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/)\n      * [Unit 2.1Pytorch Intro](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-1-introducing-pytorch/)\n      * [Unit 2.2Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-2-what-are-tensors-part-1-and-part-2/)\n      * [Unit 2.3Using Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-3-how-do-we-use-tensors-in-pytorch/)\n      * [Unit 2.4Linear Algebra](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-4-improving-code-efficiency-with-linear-algebra-parts-1-4/)\n      * [Unit 2.5Debugging Code](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-5-debugging-code/)\n      * [Unit 2.6Revisiting Perceptron w/ Tensors](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-6-revisiting-the-perceptron-algorithm/)\n      * [Unit 2.7Computation Graphs](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/2-7-seeing-predictive-models-as-computation-graphs/)\n      * [Unit 2 ExercisesExercises](https://lightning.ai/courses/deep-learning-fundamentals/2-0-unit-2-overview/unit-2-exercises/)\n    * [Unit 3Model Training in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/)\n      * [Unit 3.1Using Logistic Regression for Classification](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-1-using-logistic-regression-for-classification-parts-1-3/)\n      * [Unit 3.2The Logistic Regression Computation Graph](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-2-the-logistic-regression-computation-graph/)\n      * [Unit 3.3Model Training with Stochastic Gradient Descent ](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-3-model-training-with-stochastic-gradient-descent-part-1-4/)\n      * [Unit 3.4Automatic Differentiation in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-4-automatic-differentiation-in-pytorch/)\n      * [Unit 3.5The PyTorch API](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-5-the-pytorch-api-parts-1-2/)\n      * [Unit 3.6Training a Logistic Regression Model in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-6-training-a-logistic-regression-model-in-pytorch-parts-1-3/)\n      * [Unit 3.7 Feature Normalization](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/3-7-feature-normalization-parts-1-2/)\n      * [Unit 3 ExercisesUnit 3 Exercies](https://lightning.ai/courses/deep-learning-fundamentals/3-0-overview-model-training-in-pytorch/unit-3-exercises/)\n    * [Unit 4Training Multilayer Neural Networks Overview](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/)\n      * [Unit 4.1Logistic Regression for Multiple Classes](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/logistic-regression-for-multiple-classes-part-1-5/)\n      * [Unit 4.2Multilayer Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-2-multilayer-neural-networks-part-1-3/)\n      * [Unit 4.3Training a Multilayer Neural Network in PyTorch](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-3-training-a-multilayer-neural-network-in-pytorch-part-1-5/)\n      * [Unit 4.4Defining Efficient Data Loaders](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-4-defining-efficient-data-loaders-part-1-4/)\n      * [Unit 4.5Multilayer Neural Networks for Regression](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-5-multilayer-neural-networks-for-regression-parts-1-2/)\n      * [Unit 4.6Speeding Up Model Training Using GPUs](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/4-6-speeding-up-model-training-using-gpus/)\n      * [Unit 4 ExercisesUnit 4 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/training-multilayer-neural-networks-overview/unit-4-exercises/)\n    * [Unit 5Organizing Your Code with Lightning](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/)\n      * [Unit 5.1 Organizing Your Code with Lightning](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/organizing-your-code-with-lightning/)\n      * [Unit 5.2Training a Multilayer Perceptron using the Lightning Trainer](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-2-training-a-multilayer-perceptron-using-the-lightning-trainer/)\n      * [Unit 5.3Computing Metrics Efficiently with TorchMetrics](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-3-computing-metrics-efficiently-with-torchmetrics/)\n      * [Unit 5.4Making Code Reproducible](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-4-making-code-reproducible/)\n      * [Unit 5.5Organizing Your Data Loaders with Data Modules](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-5-organizing-your-data-loaders-with-data-modules/)\n      * [Unit 5.6The Benefits of Logging Your Model Training](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-6-the-benefits-of-logging-your-model-training/)\n      * [Unit 5.7Evaluating and Using Models on New Data](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-7-evaluating-and-using-models-on-new-data/)\n      * [Unit 5.8Add Functionality with Callbacks](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/5-8-add-functionality-with-callbacks/)\n      * [Unit 5 ExercisesUnit 5 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/overview-organizing-your-code-with-pytorch-lightning/unit-5-exercises/)\n    * [Unit 6Essential Deep Learning Tips & Tricks](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/)\n      * [Unit 6.1 Model Checkpointing and Early Stopping](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.1-model-checkpointing-and-early-stopping/)\n      * [Unit 6.2Learning Rates and Learning Rate Schedulers](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.2-learning-rates-and-learning-rate-schedulers/)\n      * [Unit 6.3Using More Advanced Optimization Algorithms](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.3-using-more-advanced-optimization-algorithms/)\n      * [Unit 6.4Choosing Activation Functions](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.4-choosing-activation-functions/)\n      * [Unit 6.5Automating The Hyperparameter Tuning Process](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.5-automating-the-hyperparameter-tuning-process/)\n      * [Unit 6.6Improving Convergence with Batch Normalization](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6.6-improving-convergence-with-batch-normalization/)\n      * [Unit 6.7Reducing Overfitting With Dropout](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/6.7-reducing-overfitting-with-dropout/)\n      * [Unit 6.8Debugging Deep Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/6.8-debugging-deep-neural-networks/)\n      * [Unit 6 ExercisesUnit 6 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-6-overview-essential-deep-learning-tips-tricks/unit-6-exercises/)\n    * [Unit 7Getting Started with Computer Vision](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/)\n      * [Unit 7.1Working With Images](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.1-working-with-images/)\n      * [Unit 7.2How Convolutional Neural Networks Work](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.2-how-convolutional-neural-networks-work/)\n      * [Unit 7.3Convolutional Neural Network Architectures](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.3-convolutional-neural-network-architectures/)\n      * [Unit 7.4Training Convolutional Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.4-training-convolutional-neural-networks/)\n      * [Unit 7.5Improving Predictions with Data Augmentation](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.5-improving-predictions-with-data-augmentation/)\n      * [Unit 7.6Leveraging Pretrained Models with Transfer Learning](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.6-leveraging-pretrained-models-with-transfer-learning/)\n      * [Unit 7.7Using Unlabeled Data with Self-Supervised](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7.7-using-unlabeled-data-with-self-supervised/)\n      * [Unit 7 ExercisesUnit 7 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-7-overview-getting-started-with-computer-vision/unit-7-exercises/)\n    * [Unit 8Natural Language Processing and Large Language Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/)\n      * [Unit 8.1Working with Text Data](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.1-working-with-text-data/)\n      * [Unit 8.2Training A Text Classifier Baseline](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.2-training-a-text-classifier-baseline/)\n      * [Unit 8.3Introduction to Recurrent Neural Networks](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.3-introduction-to-recurrent-neural-networks/)\n      * [Unit 8.4From RNNs to the Transformer Architecture](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.4-from-rnns-to-the-transformer-architecture/)\n      * [Unit 8.5Understanding Self-Attention](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.5-understanding-self-attention/)\n      * [Unit 8.6Large Language Models](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.6-large-language-models/)\n      * [Unit 8.7A Large Language Model for Classification](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/8.7-a-large-language-model-for-classification/)\n      * [Unit 8 ExercisesUnit 8 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/unit-8.0-natural-language-processing-and-large-language-models/unit-8-exercises/)\n    * [Unit 9Techniques for Speeding Up Model Training](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/)\n      * [Unit 9.1Accelerated Model Training via Mixed-Precision Training](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.1-accelerated-model-training-via-mixed-precision-training/)\n      * [Unit 9.2Multi-GPU Training Strategies](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.2-multi-gpu-training-strategies/)\n      * [Unit 9.3Deep Dive Into Data Parallelism](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.3-deep-dive-into-data-parallelism/)\n      * [Unit 9.4Compiling PyTorch Models](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.4-compiling-pytorch-models/)\n      * [Unit 9.5Increasing Batch Sizes to Increase Throughput ](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9.5-increasing-batch-sizes-to-increase-throughput/)\n      * [Unit 9 ExercisesUnit 9 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/9.0-overview-techniques-for-speeding-up-model-training/unit-9-exercises/)\n    * [Unit 10 The Finale: Our Next Steps After AI Model Training](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/)\n      * [Unit 10.1Trustworthy and Reliable Machine Learning](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.1-trustworthy-and-reliable-machine-learning/)\n      * [Unit 10.2Scaling PyTorch Models without Boilerplate Code](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.2-fabric-scaling-pytorch-models-without-boilerplate-code/)\n      * [Unit 10.3Designing Machine Learning Systems](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.3-designing-machine-learning-systems/)\n      * [Unit 10.4Conclusion](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/10.4-conclusion/)\n      * [Unit 10 ExercisesUnit 10 Exercises](https://lightning.ai/courses/deep-learning-fundamentals/10.0-overview-the-finale-our-next-steps-after-ai-model-training/unit-10-exercises/)\n\n\n[Final certification exam](https://lightning.ai/ai-education/deep-learning-fundamentals/certification/)\n[Deep Learning Fundamentals](https://lightning.ai/pages/courses/deep-learning-fundamentals/) > Deep Learning Fundamentals\n  * Share:\n  * [![Tweet](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/twitter.svg)](https://twitter.com/intent/tweet?source=http%3A%2F%2Flightning.ai&text=:%20http%3A%2F%2Flightning.ai&via=LightningAI \"Tweet\")\n  * [![Submit to Reddit](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/reddit.svg)](http://www.reddit.com/submit?url=http%3A%2F%2Flightning.ai&title= \"Submit to Reddit\")\n  * [![Share on LinkedIn](https://lightningaidev.wpengine.com/wp-content/themes/lightning-wp/assets/images/icons/linkedin.svg)](http://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flightning.ai&title=&summary=&source=http%3A%2F%2Flightning.ai \"Share on LinkedIn\")\n\n\nCourse Progress:\n#  Deep Learning Fundamentals\n[Start Course](https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n## Welcome to Deep Learning Fundamentals\nDeep Learning Fundamentals is a free course on learning deep learning using a modern open-source stack.\nIf you found this page, you probably heard that artificial intelligence and deep learning are taking the world by storm. This is correct. In this course, [Sebastian Raschka](https://sebastianraschka.com/), a best-selling author and professor, will teach you deep learning (machine learning with deep learning) from the ground up via a course of 10 units with bite-sized videos, quizzes, and exercises. The entire course is free and uses the most popular open-source tools for deep learning.\n**What will you learn in this course?**\n  * What machine learning is and when to use it\n  * The main concepts of deep learning\n  * How to design deep learning experiments with PyTorch\n  * How to write efficient deep learning code with PyTorch Lightning\n\n\n**What will you be able to do after this course?**\n  * Build classifiers for various kinds of data like tables, images, and text\n  * Tune models effectively to optimize predictive and computational performance\n\n\n**How is this course structured?**\n  * The course consists of 10 units, each containing several subsections\n  * It is centered around informative, succinct videos that are respectful of your time\n  * In each unit, you will find optional exercises to practice your knowledge\n  * We also provide additional resources for those who want a deep dive on specific topics\n\n\n**What are the prerequisites?**\n  * Ideally, you should already be familiar with programming in Python\n  * (Some lectures will involve a tiny bit of math, but a strong math background is not required!)\n\n\n**Are there interactive quizzes or exercises?**\n  * Each section is accompanied by optional multiple-choice quizzes to test your understanding of the material\n  * Optionally, each unit also features one or more code exercises to practice implementing concepts covered in this class\n\n\n**Is there a course completion badge or certificate?**\n  * At the end of this course, you can take an optional exam featuring 25 multiple-choice questions\n  * Upon answering 80% of the questions in the exam correctly (there are 5 attempts), you obtain an optional course completion badge that can be shared on LinkedIn\n\n[Start Course](https://lightning.ai/pages/courses/deep-learning-fundamentals/unit-1/unit-1-1/)\n**Log in or create a free Lightning.ai account to access:**\n  * Quizzes\n  * Completion badges\n  * Progress tracking\n  * Additional downloadable content\n  * Additional AI education resources\n  * Notifications when new units are released\n  * Free cloud computing credits\n\n[Sign Up or Log In](https://lightning.ai/sign-in?redirectTo=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F)\n#####  Watch Video 1\n##### Videos\n![](https://t.co/1/i/adsct?=&bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%261080%26600%262%2624%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=ced468f8-e912-4e7d-88f9-ca7345132140&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=8c30a3f6-c71f-4138-93b4-02957c8b251a&tw_document_href=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.33)![](https://analytics.twitter.com/1/i/adsct?=&bci=4&dv=UTC%26en-US%26Google%20Inc.%26Linux%20x86_64%26255%261080%26600%262%2624%261080%26600%260%26na&eci=3&event=%7B%22%22%3A%22%22%7D&event_id=ced468f8-e912-4e7d-88f9-ca7345132140&integration=gtm&p_id=Twitter&p_user_id=0&pl_id=8c30a3f6-c71f-4138-93b4-02957c8b251a&tw_document_href=https%3A%2F%2Flightning.ai%2Fcourses%2Fdeep-learning-fundamentals%2F&tw_iframe_status=0&txn_id=p06ii&type=javascript&version=2.3.33)\n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "Markdown(documents[0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Filter the documents by a predefined Quality Score"
      ],
      "metadata": {
        "id": "vHOv_kDupR7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def filter_by_quality(\n",
        "    documents: list[Document],\n",
        "    content_quality_score_threshold: float,\n",
        ") -> Annotated[list[Document], \"filtered_documents\"]:\n",
        "    assert 0 <= content_quality_score_threshold <= 1, (\n",
        "        \"Content quality score threshold must be between 0 and 1\"\n",
        "    )\n",
        "\n",
        "    valid_docs = [\n",
        "        doc\n",
        "        for doc in documents\n",
        "        if not doc.content_quality_score\n",
        "        or doc.content_quality_score > content_quality_score_threshold\n",
        "    ]\n",
        "\n",
        "    return valid_docs"
      ],
      "metadata": {
        "id": "wsrdJEdYykuc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from typing import Any, Generator, Literal, Union\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain_core.documents import Document as LangChainDocument\n",
        "from langchain_mongodb.retrievers import (\n",
        "    MongoDBAtlasParentDocumentRetriever,\n",
        ")\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "GIsUR0vuzHbo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define embedding models - either openai or Huggingface"
      ],
      "metadata": {
        "id": "Oe-9TpPIpieW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EmbeddingModelType = Literal[\"openai\", \"huggingface\"]\n",
        "EmbeddingsModel = Union[OpenAIEmbeddings, HuggingFaceEmbeddings]\n",
        "\n",
        "\n",
        "def get_embedding_model(\n",
        "    model_id: str,\n",
        "    model_type: EmbeddingModelType = \"huggingface\",\n",
        "    device: str = \"cpu\",\n",
        ") -> EmbeddingsModel:\n",
        "\n",
        "    if model_type == \"openai\":\n",
        "        return get_openai_embedding_model(model_id)\n",
        "    elif model_type == \"huggingface\":\n",
        "        return get_huggingface_embedding_model(model_id, device)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid embedding model type: {model_type}\")\n",
        "\n",
        "\n",
        "def get_openai_embedding_model(model_id: str) -> OpenAIEmbeddings:\n",
        "    return OpenAIEmbeddings(\n",
        "        model=model_id,\n",
        "        allowed_special={\"<|endoftext|>\"},\n",
        "    )\n",
        "\n",
        "\n",
        "def get_huggingface_embedding_model(\n",
        "    model_id: str, device: str\n",
        ") -> HuggingFaceEmbeddings:\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=model_id,\n",
        "        model_kwargs={\"device\": device, \"trust_remote_code\": True},\n",
        "        encode_kwargs={\"normalize_embeddings\": False},\n",
        "    )"
      ],
      "metadata": {
        "id": "k8xy_Ld5ziHp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual summary using OpenAI"
      ],
      "metadata": {
        "id": "RIGKi7tpVF19"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncio\n",
        "import os\n",
        "\n",
        "import psutil\n",
        "from litellm import acompletion\n",
        "from loguru import logger\n",
        "from openai import AsyncOpenAI\n",
        "from pydantic import BaseModel\n",
        "from tqdm.asyncio import tqdm\n",
        "\n",
        "HUGGINGFACE_DEDICATED_ENDPOINT = \"meta-llama/Llama-4-Maverick-17B-128E-Instruct\"\n",
        "HUGGINGFACE_ACCESS_TOKEN = userdata.get('HF_TOKEN')\n",
        "\n",
        "class ContextualDocument(BaseModel):\n",
        "    content: str\n",
        "    chunk: str | None = None\n",
        "    contextual_summarization: str | None = None\n",
        "\n",
        "    def add_contextual_summarization(self, summary: str) -> \"ContextualDocument\":\n",
        "        self.contextual_summarization = summary\n",
        "        return self\n",
        "\n",
        "\n",
        "class ContextualSummarizationAgent:\n",
        "\n",
        "    SYSTEM_PROMPT_TEMPLATE = \"\"\"You are a helpful assistant specialized in summarizing documents relative to a given chunk.\n",
        "    <document>\n",
        "    {content}\n",
        "    </document>\n",
        "    Here is the chunk we want to situate within the whole document\n",
        "    <chunk>\n",
        "    {chunk}\n",
        "    </chunk>\n",
        "    Please give a short succinct context of maximum {characters} characters to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"gpt-4o-mini\",\n",
        "        max_characters: int = 128,\n",
        "        mock: bool = False,\n",
        "        max_concurrent_requests: int = 4,\n",
        "    ) -> None:\n",
        "        self.model_id = model_id\n",
        "        self.max_characters = max_characters\n",
        "        self.mock = mock\n",
        "        self.max_concurrent_requests = max_concurrent_requests\n",
        "\n",
        "    def __call__(self, content: str, chunks: list[str]) -> list[str]:\n",
        "        try:\n",
        "            loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            results = asyncio.run(self.__summarize_context_batch(content, chunks))\n",
        "        else:\n",
        "            results = loop.run_until_complete(\n",
        "                self.__summarize_context_batch(content, chunks)\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context_batch(\n",
        "        self, content: str, chunks: list[str]\n",
        "    ) -> list[str]:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        start_mem = process.memory_info().rss\n",
        "        total_chunks = len(chunks)\n",
        "        logger.debug(\n",
        "            f\"Starting contextual summarization for {total_chunks} chunks with {self.max_concurrent_requests} concurrent requests. \"\n",
        "            f\"Initial memory usage: {start_mem // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        documents = [\n",
        "            ContextualDocument(content=content, chunk=chunk) for chunk in chunks\n",
        "        ]\n",
        "\n",
        "        summarized_documents = await self.__process_batch(\n",
        "            documents, await_time_seconds=7\n",
        "        )\n",
        "        documents_with_summaries = [\n",
        "            doc\n",
        "            for doc in summarized_documents\n",
        "            if doc.contextual_summarization is not None\n",
        "        ]\n",
        "        documents_without_summaries = [\n",
        "            doc for doc in documents if doc.contextual_summarization is None\n",
        "        ]\n",
        "\n",
        "        # Retry failed documents with increased await time\n",
        "        if documents_without_summaries:\n",
        "            logger.info(\n",
        "                f\"Retrying {len(documents_without_summaries)} failed documents with increased await time...\"\n",
        "            )\n",
        "            retry_results = await self.__process_batch(\n",
        "                documents_without_summaries, await_time_seconds=20\n",
        "            )\n",
        "            documents_with_summaries += retry_results\n",
        "\n",
        "        end_mem = process.memory_info().rss\n",
        "        memory_diff = end_mem - start_mem\n",
        "        logger.debug(\n",
        "            f\"Contextual summarization completed. \"\n",
        "            f\"Final memory usage: {end_mem // (1024 * 1024)} MB, \"\n",
        "            f\"Memory difference: {memory_diff // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        success_count = len(documents_with_summaries)\n",
        "        failed_count = total_chunks - success_count\n",
        "        logger.info(\n",
        "            f\"Contextual summarization results: \"\n",
        "            f\"{success_count}/{total_chunks} chunks summarized successfully ✓ | \"\n",
        "            f\"{failed_count}/{total_chunks} chunks failed ✗\"\n",
        "        )\n",
        "\n",
        "        contextual_chunks = []\n",
        "        for doc in documents_with_summaries:\n",
        "            if doc.contextual_summarization is not None:\n",
        "                chunk = f\"{doc.contextual_summarization}\\n\\n{doc.chunk}\"\n",
        "            else:\n",
        "                chunk = f\"{doc.chunk}\"\n",
        "\n",
        "            contextual_chunks.append(chunk)\n",
        "\n",
        "        return contextual_chunks\n",
        "\n",
        "    async def __process_batch(\n",
        "        self, documents: list[ContextualDocument], await_time_seconds: int\n",
        "    ) -> list[ContextualDocument]:\n",
        "\n",
        "        semaphore = asyncio.Semaphore(self.max_concurrent_requests)\n",
        "        tasks = [\n",
        "            self.__summarize_context(\n",
        "                document, semaphore, await_time_seconds=await_time_seconds\n",
        "            )\n",
        "            for document in documents\n",
        "        ]\n",
        "        results = []\n",
        "        for coro in tqdm(\n",
        "            asyncio.as_completed(tasks),\n",
        "            total=len(documents),\n",
        "            desc=\"Processing documents\",\n",
        "            unit=\"doc\",\n",
        "        ):\n",
        "            result = await coro\n",
        "            results.append(result)\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context(\n",
        "        self,\n",
        "        document: ContextualDocument,\n",
        "        semaphore: asyncio.Semaphore | None = None,\n",
        "        await_time_seconds: int = 2,\n",
        "    ) -> ContextualDocument:\n",
        "        if self.mock:\n",
        "            return document.add_contextual_summarization(\"This is a mock summary\")\n",
        "\n",
        "        async def process_document() -> ContextualDocument:\n",
        "            try:\n",
        "                response = await acompletion(\n",
        "                    model=self.model_id,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": self.SYSTEM_PROMPT_TEMPLATE.format(\n",
        "                                characters=self.max_characters,\n",
        "                                content=document.content[\n",
        "                                    :6000\n",
        "                                ],  # Keep it short to lower latency and costs.\n",
        "                                chunk=document.chunk,\n",
        "                            ),\n",
        "                        },\n",
        "                    ],\n",
        "                    stream=False,\n",
        "                    temperature=0,\n",
        "                )\n",
        "                await asyncio.sleep(await_time_seconds)  # Rate limiting\n",
        "\n",
        "                if not response.choices:\n",
        "                    logger.warning(\"No contextual summary generated for chunk\")\n",
        "                    return document\n",
        "\n",
        "                context_summary: str = response.choices[0].message.content\n",
        "                return document.add_contextual_summarization(context_summary)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to generate contextual summary: {str(e)}\")\n",
        "                return document\n",
        "\n",
        "        if semaphore:\n",
        "            async with semaphore:\n",
        "                return await process_document()\n",
        "\n",
        "        return await process_document()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kIp2qn5F0BpQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contextual summary using HuggingFace LLama-4"
      ],
      "metadata": {
        "id": "aVM5JW4hVOBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleSummarizationAgent:\n",
        "\n",
        "    SYSTEM_PROMPT_TEMPLATE = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "    ### Instruction:\n",
        "    You are a helpful assistant specialized in summarizing documents for the purposes of improving semantic and keyword search retrieval.\n",
        "    Generate a concise TL;DR summary in plain text format having a maximum of {characters} characters of the key findings from the provided documents,\n",
        "    highlighting the most significant insights. Answer only with the succinct context and nothing else.\n",
        "\n",
        "    ### Input:\n",
        "    {content}\n",
        "\n",
        "    ### Response:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_id: str = \"gpt-4o-mini\",\n",
        "        base_url: str | None = HUGGINGFACE_DEDICATED_ENDPOINT,\n",
        "        api_key: str | None = HUGGINGFACE_ACCESS_TOKEN,\n",
        "        max_characters: int = 128,\n",
        "        mock: bool = False,\n",
        "        max_concurrent_requests: int = 4,\n",
        "    ) -> None:\n",
        "        self.model_id = model_id\n",
        "        self.base_url = base_url\n",
        "        self.api_key = api_key\n",
        "        self.max_characters = max_characters\n",
        "        self.mock = mock\n",
        "        self.max_concurrent_requests = max_concurrent_requests\n",
        "\n",
        "        if self.model_id == \"tgi\":\n",
        "            assert self.base_url and self.api_key, (\n",
        "                \"Base URL and API key are required for TGI Hugging Face Dedicated Endpoint\"\n",
        "            )\n",
        "\n",
        "            self.client = AsyncOpenAI(\n",
        "                base_url=self.base_url,\n",
        "                api_key=self.api_key,\n",
        "            )\n",
        "        else:\n",
        "            self.client = AsyncOpenAI()\n",
        "\n",
        "    def __call__(self, content: str, chunks: list[str]) -> list[str]:\n",
        "        try:\n",
        "            loop = asyncio.get_running_loop()\n",
        "        except RuntimeError:\n",
        "            results = asyncio.run(self.__summarize_context_batch(content, chunks))\n",
        "        else:\n",
        "            results = loop.run_until_complete(\n",
        "                self.__summarize_context_batch(content, chunks)\n",
        "            )\n",
        "\n",
        "        return results\n",
        "\n",
        "    async def __summarize_context_batch(\n",
        "        self, content: str, chunks: list[str]\n",
        "    ) -> list[str]:\n",
        "        process = psutil.Process(os.getpid())\n",
        "        start_mem = process.memory_info().rss\n",
        "        logger.debug(\n",
        "            f\"Starting summarizing document.\"\n",
        "            f\"Initial memory usage: {start_mem // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        document = await self.__summarize(\n",
        "            document=ContextualDocument(content=content), await_time_seconds=20\n",
        "        )\n",
        "\n",
        "        end_mem = process.memory_info().rss\n",
        "        memory_diff = end_mem - start_mem\n",
        "        logger.debug(\n",
        "            f\"Summarization completed. \"\n",
        "            f\"Final memory usage: {end_mem // (1024 * 1024)} MB, \"\n",
        "            f\"Memory difference: {memory_diff // (1024 * 1024)} MB\"\n",
        "        )\n",
        "\n",
        "        contextual_chunks = []\n",
        "        for chunk in chunks:\n",
        "            if document.contextual_summarization is not None:\n",
        "                chunk = f\"{document.contextual_summarization}\\n\\n{chunk}\"\n",
        "            else:\n",
        "                chunk = f\"{chunk}\"\n",
        "\n",
        "            contextual_chunks.append(chunk)\n",
        "\n",
        "        return contextual_chunks\n",
        "\n",
        "    async def __summarize(\n",
        "        self,\n",
        "        document: ContextualDocument,\n",
        "        await_time_seconds: int = 2,\n",
        "    ) -> ContextualDocument:\n",
        "        if self.mock:\n",
        "            return document.add_contextual_summarization(\"This is a mock summary\")\n",
        "\n",
        "        async def process_document() -> ContextualDocument:\n",
        "            try:\n",
        "                response = await self.client.chat.completions.create(\n",
        "                    model=self.model_id,\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"system\",\n",
        "                            \"content\": self.SYSTEM_PROMPT_TEMPLATE.format(\n",
        "                                characters=self.max_characters, content=document.content\n",
        "                            ),\n",
        "                        },\n",
        "                    ],\n",
        "                    stream=False,\n",
        "                    temperature=0,\n",
        "                )\n",
        "                await asyncio.sleep(await_time_seconds)  # Rate limiting\n",
        "\n",
        "                if not response.choices:\n",
        "                    logger.warning(\"No contextual summary generated for chunk\")\n",
        "                    return document\n",
        "\n",
        "                context_summary: str = response.choices[0].message.content\n",
        "                return document.add_contextual_summarization(context_summary)\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to generate contextual summary: {str(e)}\")\n",
        "                return document\n",
        "\n",
        "        return await process_document()"
      ],
      "metadata": {
        "id": "1oWi2dz8UO2g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunk dataset"
      ],
      "metadata": {
        "id": "8yM_-wmhVUZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Callable, Literal, Union\n",
        "\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from loguru import logger\n",
        "\n",
        "# Add type definitions at the top of the file\n",
        "SummarizationType = Literal[\"contextual\", \"simple\", \"none\"]\n",
        "SummarizationAgent = Union[ContextualSummarizationAgent, SimpleSummarizationAgent]\n",
        "\n",
        "\n",
        "def get_splitter(\n",
        "    chunk_size: int, summarization_type: SummarizationType = \"none\", **kwargs\n",
        ") -> RecursiveCharacterTextSplitter:\n",
        "    chunk_overlap = int(0.15 * chunk_size)\n",
        "\n",
        "    if summarization_type == \"none\":\n",
        "        return RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "            encoding_name=\"cl100k_base\",\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "        )\n",
        "\n",
        "    if summarization_type == \"contextual\":\n",
        "        handler = ContextualSummarizationAgent(**kwargs)\n",
        "    elif summarization_type == \"simple\":\n",
        "        handler = SimpleSummarizationAgent(**kwargs)\n",
        "\n",
        "    return HandlerRecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "        encoding_name=\"cl100k_base\",\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        handler=handler,\n",
        "    )\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "X1yRD_Z7zy0l"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HandlerRecursiveCharacterTextSplitter(RecursiveCharacterTextSplitter):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        handler: Callable[[str, list[str]], list[str]] | None = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.handler = handler if handler is not None else lambda _, x: x\n",
        "\n",
        "    def split_text(self, text: str) -> list[str]:\n",
        "        chunks = super().split_text(text)\n",
        "        parsed_chunks = self.handler(text, chunks)\n",
        "\n",
        "        return parsed_chunks"
      ],
      "metadata": {
        "id": "lIS5MxI1p6C_"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Retrievers"
      ],
      "metadata": {
        "id": "WDGBV2g8VzQV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal, Union\n",
        "\n",
        "from langchain_mongodb import MongoDBAtlasVectorSearch\n",
        "from langchain_mongodb.retrievers import (\n",
        "    MongoDBAtlasHybridSearchRetriever,\n",
        "    MongoDBAtlasParentDocumentRetriever,\n",
        ")\n",
        "\n",
        "# Add these type definitions at the top of the file\n",
        "RetrieverType = Literal[\"contextual\", \"parent\"]\n",
        "RetrieverModel = Union[\n",
        "    MongoDBAtlasHybridSearchRetriever, MongoDBAtlasParentDocumentRetriever\n",
        "]\n",
        "\n",
        "def get_hybrid_search_retriever(\n",
        "    embedding_model: EmbeddingsModel, k: int\n",
        ") -> MongoDBAtlasHybridSearchRetriever:\n",
        "    vectorstore = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "        connection_string=MONGODB_URI,\n",
        "        embedding=embedding_model,\n",
        "        namespace=f\"{MONGODB_DATABASE_NAME}.rag\",\n",
        "        text_key=\"chunk\",\n",
        "        embedding_key=\"embedding\",\n",
        "        relevance_score_fn=\"dotProduct\",\n",
        "    )\n",
        "\n",
        "    retriever = MongoDBAtlasHybridSearchRetriever(\n",
        "        vectorstore=vectorstore,\n",
        "        search_index_name=\"chunk_text_search\",\n",
        "        top_k=k,\n",
        "        vector_penalty=50,\n",
        "        fulltext_penalty=50,\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def get_parent_document_retriever(\n",
        "    embedding_model: EmbeddingsModel, k: int = 3\n",
        ") -> MongoDBAtlasParentDocumentRetriever:\n",
        "    retriever = MongoDBAtlasParentDocumentRetriever.from_connection_string(\n",
        "        connection_string=MONGODB_URI,\n",
        "        embedding_model=embedding_model,\n",
        "        child_splitter=get_splitter(200),\n",
        "        parent_splitter=get_splitter(800),\n",
        "        database_name=MONGODB_DATABASE_NAME,\n",
        "        collection_name=\"rag\",\n",
        "        text_key=\"page_content\",\n",
        "        search_kwargs={\"k\": k},\n",
        "    )\n",
        "\n",
        "    return retriever"
      ],
      "metadata": {
        "id": "h2VWJGebzYI0"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_docs(\n",
        "    retriever: Any,\n",
        "    docs: list[LangChainDocument],\n",
        "    splitter: RecursiveCharacterTextSplitter,\n",
        "    batch_size: int = 4,\n",
        "    max_workers: int = 2,\n",
        ") -> list[None]:\n",
        "    batches = list(get_batches(docs, batch_size))\n",
        "    results = []\n",
        "    total_docs = len(docs)\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = [\n",
        "            executor.submit(process_batch, retriever, batch, splitter)\n",
        "            for batch in batches\n",
        "        ]\n",
        "\n",
        "        with tqdm(total=total_docs, desc=\"Processing documents\") as pbar:\n",
        "            for future in as_completed(futures):\n",
        "                result = future.result()\n",
        "                results.append(result)\n",
        "                pbar.update(batch_size)\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7Ap5byvLzCVg"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(\n",
        "    docs: list[LangChainDocument], batch_size: int\n",
        ") -> Generator[list[LangChainDocument], None, None]:\n",
        "    for i in range(0, len(docs), batch_size):\n",
        "        yield docs[i : i + batch_size]\n"
      ],
      "metadata": {
        "id": "ZGctK5hKqG5t"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_batch(\n",
        "    retriever: Any,\n",
        "    batch: list[LangChainDocument],\n",
        "    splitter: RecursiveCharacterTextSplitter,\n",
        ") -> None:\n",
        "    try:\n",
        "        if isinstance(retriever, MongoDBAtlasParentDocumentRetriever):\n",
        "            retriever.add_documents(batch)\n",
        "        else:\n",
        "            split_docs = splitter.split_documents(batch)\n",
        "            retriever.vectorstore.add_documents(split_docs)\n",
        "\n",
        "        logger.info(f\"Successfully processed {len(batch)} documents.\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error processing batch of {len(batch)} documents: {str(e)}\")"
      ],
      "metadata": {
        "id": "w-TgA0fDqFir"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_retriever(\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType = \"huggingface\",\n",
        "    retriever_type: RetrieverType = \"contextual\",\n",
        "    k: int = 3,\n",
        "    device: str = \"cpu\",\n",
        ") -> RetrieverModel:\n",
        "    embedding_model = get_embedding_model(\n",
        "        embedding_model_id, embedding_model_type, device\n",
        "    )\n",
        "\n",
        "    if retriever_type == \"contextual\":\n",
        "        return get_hybrid_search_retriever(embedding_model, k)\n",
        "    elif retriever_type == \"parent\":\n",
        "        return get_parent_document_retriever(embedding_model, k)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid retriever type: {retriever_type}\")"
      ],
      "metadata": {
        "id": "-bFwYkZcYizO"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mongodb.index import create_fulltext_search_index\n",
        "\n",
        "class MongoDBIndex:\n",
        "    def __init__(\n",
        "        self,\n",
        "        retriever,\n",
        "        mongodb_client: MongoDBService,\n",
        "    ) -> None:\n",
        "        self.retriever = retriever\n",
        "        self.mongodb_client = mongodb_client\n",
        "\n",
        "    def create(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        is_hybrid: bool = False,\n",
        "    ) -> None:\n",
        "        vectorstore = self.retriever.vectorstore\n",
        "\n",
        "        vectorstore.create_vector_search_index(\n",
        "            dimensions=embedding_dim,\n",
        "        )\n",
        "        if is_hybrid:\n",
        "            create_fulltext_search_index(\n",
        "                collection=self.mongodb_client.collection,\n",
        "                field=vectorstore._text_key,\n",
        "                index_name=self.retriever.search_index_name,\n",
        "            )"
      ],
      "metadata": {
        "id": "Lo-YmI1ZYzA-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "r97hwOLh07tf"
      },
      "outputs": [],
      "source": [
        "def chunk_embed_load(\n",
        "    documents: list[Document],\n",
        "    collection_name: str,\n",
        "    processing_batch_size: int,\n",
        "    processing_max_workers: int,\n",
        "    retriever_type: RetrieverType,\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType,\n",
        "    embedding_model_dim: int,\n",
        "    chunk_size: int,\n",
        "    contextual_summarization_type: SummarizationType = \"none\",\n",
        "    contextual_agent_model_id: str | None = None,\n",
        "    contextual_agent_max_characters: int | None = None,\n",
        "    mock: bool = False,\n",
        "    device: str = \"cpu\",\n",
        ") -> None:\n",
        "\n",
        "    retriever = get_retriever(\n",
        "        embedding_model_id=embedding_model_id,\n",
        "        embedding_model_type=embedding_model_type,\n",
        "        retriever_type=retriever_type,\n",
        "        device=device,\n",
        "    )\n",
        "    splitter = get_splitter(\n",
        "        chunk_size=chunk_size,\n",
        "        summarization_type=contextual_summarization_type,\n",
        "        model_id=contextual_agent_model_id,\n",
        "        max_characters=contextual_agent_max_characters,\n",
        "        mock=mock,\n",
        "        max_concurrent_requests=processing_max_workers,\n",
        "    )\n",
        "\n",
        "    with MongoDBService(\n",
        "        model=Document, collection_name=collection_name\n",
        "    ) as mongodb_client:\n",
        "        mongodb_client.clear_collection()\n",
        "\n",
        "        docs = [\n",
        "            LangChainDocument(\n",
        "                page_content=doc.content, metadata=doc.metadata.model_dump()\n",
        "            )\n",
        "            for doc in documents\n",
        "            if doc\n",
        "        ]\n",
        "        process_docs(\n",
        "            retriever,\n",
        "            docs,\n",
        "            splitter=splitter,\n",
        "            batch_size=processing_batch_size,\n",
        "            max_workers=processing_max_workers,\n",
        "        )\n",
        "\n",
        "        index = MongoDBIndex(\n",
        "            retriever=retriever,\n",
        "            mongodb_client=mongodb_client,\n",
        "        )\n",
        "        index.create(\n",
        "            embedding_dim=embedding_model_dim,\n",
        "            is_hybrid=retriever_type == \"contextual\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "CYeNem9l10wG"
      },
      "outputs": [],
      "source": [
        "extract_collection_name = COLLECTION_NAME\n",
        "fetch_limit = 30\n",
        "load_collection_name = 'rag'\n",
        "content_quality_score_threshold = 0.6\n",
        "retriever_type = 'contextual'\n",
        "embedding_model_id  = 'text-embedding-3-small'\n",
        "embedding_model_type = 'openai'\n",
        "embedding_model_dim = 1536\n",
        "chunk_size = 3072\n",
        "contextual_summarization_type = 'contextual'\n",
        "contextual_agent_model_id = 'gpt-4o-mini'\n",
        "contextual_agent_max_characters = 128\n",
        "mock = False\n",
        "processing_batch_size = 2\n",
        "processing_max_workers = 2\n",
        "device = 'cpu' # or cuda (for Nvidia GPUs) or mps (for Apple M1/M2/M3 chips)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "NZjBHhqFDb3t"
      },
      "outputs": [],
      "source": [
        "def compute_rag_vector_index(\n",
        "    extract_collection_name: str,\n",
        "    fetch_limit: int,\n",
        "    load_collection_name: str,\n",
        "    content_quality_score_threshold: float,\n",
        "    retriever_type: RetrieverType,\n",
        "    embedding_model_id: str,\n",
        "    embedding_model_type: EmbeddingModelType,\n",
        "    embedding_model_dim: int,\n",
        "    chunk_size: int,\n",
        "    contextual_summarization_type: SummarizationType = \"none\",\n",
        "    contextual_agent_model_id: str | None = None,\n",
        "    contextual_agent_max_characters: int | None = None,\n",
        "    mock: bool = False,\n",
        "    processing_batch_size: int = 256,\n",
        "    processing_max_workers: int = 10,\n",
        "    device: str = \"cpu\",\n",
        ") -> None:\n",
        "    documents = fetch_from_mongodb(\n",
        "        collection_name=extract_collection_name, limit=fetch_limit\n",
        "    )\n",
        "\n",
        "    documents = documents[:8]\n",
        "\n",
        "    documents = filter_by_quality(\n",
        "        documents=documents,\n",
        "        content_quality_score_threshold=content_quality_score_threshold,\n",
        "    )\n",
        "    chunk_embed_load(\n",
        "        documents=documents,\n",
        "        collection_name=load_collection_name,\n",
        "        processing_batch_size=processing_batch_size,\n",
        "        processing_max_workers=processing_max_workers,\n",
        "        retriever_type=retriever_type,\n",
        "        embedding_model_id=embedding_model_id,\n",
        "        embedding_model_type=embedding_model_type,\n",
        "        embedding_model_dim=embedding_model_dim,\n",
        "        chunk_size=chunk_size,\n",
        "        contextual_summarization_type=contextual_summarization_type,\n",
        "        contextual_agent_model_id=contextual_agent_model_id,\n",
        "        contextual_agent_max_characters=contextual_agent_max_characters,\n",
        "        mock=mock,\n",
        "        device=device,\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "compute_rag_vector_index(extract_collection_name = COLLECTION_NAME,\n",
        "                        fetch_limit = 30,\n",
        "                        load_collection_name = 'rag',\n",
        "                        content_quality_score_threshold = 0.1,\n",
        "                        retriever_type = 'contextual',\n",
        "                        embedding_model_id  = 'text-embedding-3-small',\n",
        "                        embedding_model_type = 'openai',\n",
        "                        embedding_model_dim = 1536,\n",
        "                        chunk_size = 3072,\n",
        "                        contextual_summarization_type = 'contextual',\n",
        "                        contextual_agent_model_id = 'gpt-4o-mini',\n",
        "                        contextual_agent_max_characters = 128,\n",
        "                        mock = False,\n",
        "                        processing_batch_size = 2,\n",
        "                        processing_max_workers = 2,\n",
        "                        device = 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05BT2xCzXsoJ",
        "outputId": "383ed664-862a-4388-9b8e-4fe5a2903a61"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m2025-06-06 08:38:49.788\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_splitter\u001b[0m:\u001b[36m16\u001b[0m - \u001b[1mGetting splitter with chunk size: 3072 and overlap: 460\u001b[0m\n",
            "Processing documents:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[32m2025-06-06 08:38:50.820\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 428 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\u001b[32m2025-06-06 08:38:50.865\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 3 chunks with 2 concurrent requests. Initial memory usage: 428 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/3 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.83s/doc]\n",
            "\u001b[32m2025-06-06 08:38:58.661\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 430 MB, Memory difference: 2 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:38:58.663\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:38:58.674\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 2 chunks with 2 concurrent requests. Initial memory usage: 430 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/2 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  33%|███▎      | 1/3 [00:08<00:17,  8.63s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  67%|██████▋   | 2/3 [00:11<00:04,  4.99s/doc]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 2/2 [00:07<00:00,  3.94s/doc]\n",
            "\u001b[32m2025-06-06 08:39:06.556\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 431 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:06.556\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 2/2 chunks summarized successfully ✓ | 0/2 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents: 100%|██████████| 3/3 [00:16<00:00,  5.66s/doc]\n",
            "\u001b[32m2025-06-06 08:39:07.858\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 431 MB, Memory difference: 3 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:07.859\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 3/3 chunks summarized successfully ✓ | 0/3 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:07.866\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 431 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\u001b[32m2025-06-06 08:39:08.708\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  25%|██▌       | 2/8 [00:17<00:53,  8.95s/it]\u001b[32m2025-06-06 08:39:08.748\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 7 chunks with 2 concurrent requests. Initial memory usage: 432 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/7 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.86s/doc]\n",
            "\u001b[32m2025-06-06 08:39:15.731\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 432 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:15.732\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  14%|█▍        | 1/7 [00:08<00:48,  8.10s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:16.913\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  50%|█████     | 4/8 [00:26<00:24,  6.10s/it]\u001b[32m2025-06-06 08:39:16.918\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 1 chunks with 2 concurrent requests. Initial memory usage: 433 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/1 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  29%|██▊       | 2/7 [00:10<00:23,  4.63s/doc]\u001b[A\u001b[A\n",
            "Processing documents: 100%|██████████| 1/1 [00:07<00:00,  7.60s/doc]\n",
            "\u001b[32m2025-06-06 08:39:24.525\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 433 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:24.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 1/1 chunks summarized successfully ✓ | 0/1 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  43%|████▎     | 3/7 [00:15<00:20,  5.03s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:24.600\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 6 chunks with 2 concurrent requests. Initial memory usage: 433 MB\u001b[0m\n",
            "\n",
            "Processing documents:   0%|          | 0/6 [00:00<?, ?doc/s]\u001b[A\n",
            "\n",
            "Processing documents:  57%|█████▋    | 4/7 [00:17<00:11,  3.89s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  71%|███████▏  | 5/7 [00:23<00:08,  4.46s/doc]\u001b[A\u001b[A\n",
            "Processing documents:  17%|█▋        | 1/6 [00:07<00:38,  7.73s/doc]\u001b[A\n",
            "\n",
            "Processing documents:  86%|████████▌ | 6/7 [00:25<00:03,  3.65s/doc]\u001b[A\u001b[A\n",
            "Processing documents:  50%|█████     | 3/6 [00:15<00:14,  4.76s/doc]\u001b[A\n",
            "\n",
            "Processing documents: 100%|██████████| 7/7 [00:31<00:00,  4.48s/doc]\n",
            "\u001b[32m2025-06-06 08:39:40.128\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 433 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:40.130\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 7/7 chunks summarized successfully ✓ | 0/7 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:40.178\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m67\u001b[0m - \u001b[34m\u001b[1mStarting contextual summarization for 8 chunks with 2 concurrent requests. Initial memory usage: 434 MB\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:   0%|          | 0/8 [00:00<?, ?doc/s]\u001b[A\u001b[A\n",
            "Processing documents:  83%|████████▎ | 5/6 [00:22<00:04,  4.28s/doc]\u001b[A\n",
            "Processing documents: 100%|██████████| 6/6 [00:22<00:00,  3.83s/doc]\n",
            "\u001b[32m2025-06-06 08:39:47.582\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 434 MB, Memory difference: 0 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:39:47.582\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 6/6 chunks summarized successfully ✓ | 0/6 chunks failed ✗\u001b[0m\n",
            "\n",
            "\n",
            "Processing documents:  12%|█▎        | 1/8 [00:07<00:53,  7.61s/doc]\u001b[A\u001b[A\u001b[32m2025-06-06 08:39:48.842\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents:  75%|███████▌  | 6/8 [00:58<00:21, 10.60s/it]\n",
            "\n",
            "Processing documents:  38%|███▊      | 3/8 [00:15<00:24,  4.83s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents:  62%|██████▎   | 5/8 [00:22<00:12,  4.28s/doc]\u001b[A\u001b[A\n",
            "\n",
            "Processing documents: 100%|██████████| 8/8 [00:30<00:00,  3.81s/doc]\n",
            "\u001b[32m2025-06-06 08:40:10.626\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m100\u001b[0m - \u001b[34m\u001b[1mContextual summarization completed. Final memory usage: 435 MB, Memory difference: 1 MB\u001b[0m\n",
            "\u001b[32m2025-06-06 08:40:10.627\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__summarize_context_batch\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mContextual summarization results: 8/8 chunks summarized successfully ✓ | 0/8 chunks failed ✗\u001b[0m\n",
            "\u001b[32m2025-06-06 08:40:11.964\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_batch\u001b[0m:\u001b[36m13\u001b[0m - \u001b[1mSuccessfully processed 2 documents.\u001b[0m\n",
            "Processing documents: 100%|██████████| 8/8 [01:21<00:00, 10.14s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuzaWbobYAFl"
      },
      "execution_count": 55,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1TELD2WAVersnpXQaDCGzbP3UGZkOzicS",
      "authorship_tag": "ABX9TyMM/5OpQ4/XAsOwIkqf6NTi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}